# Lecture 1 - Big Data Fundamentals

---

## Contents
- Definitions and Basic Concepts
- Big Data Examples
- Big Data Technologies
- MapReduce – Basics

---

## Definitions and Basic Concepts

To begin our study of Big Data, it's essential to first grasp the sheer scale of information we are dealing with. The term "Big Data" isn't just a buzzword; it represents a fundamental shift in the volume and complexity of data being generated and collected.

A statistic from 2018 illustrates this point: every single day, approximately **2.5 quintillion (10¹⁸) bytes** of new data were created. This explosion of data comes from countless sources, including over a billion Instagram users and 2.2 billion Facebook users at the time. It's important to note that these figures don't even include the massive streams of data generated by the Internet of Things (IoT).

Eric Schmidt, former CEO of Google, provided a powerful perspective on this in 2010:
> "There was 5 exabytes of information created between the dawn of civilization through 2003, but that much information is now created every 2 days, and the pace is increasing!"

This highlights the exponential growth and the challenge it presents for traditional data processing methods.

### Formal Definitions

Given this context, how do we formally define Big Data?

- **Wikipedia** provides a comprehensive definition, describing Big Data as "data sets that are too large or complex for traditional data-processing application software to adequately deal with." It highlights key challenges such as **data capture, storage, analysis, search, sharing, transfer, and visualization**. Originally, Big Data was characterized by three concepts: **volume, variety, and velocity**. Later, concepts like **veracity** (data quality and noise) and **value** (the potential insights derived from the data) were added.

- **Gartner**, an influential research firm, defines Big Data as "high-volume, high-velocity and/or high-variety information assets that demand cost-effective, innovative forms of information processing that enable enhanced insight, decision making, and process automation."

### Common Characteristics

Synthesizing these definitions, we can identify a common ground of characteristics for what constitutes Big Data:
- **Huge data volume:** The sheer quantity of data is beyond the capacity of standard databases.
- **Unstructured data:** The data does not fit neatly into the rows and columns of a traditional relational database. It can be text, images, videos, sensor data, etc.
- **Unspecific/uncoordinated data:** Data often comes from multiple, disparate sources without a predefined schema or organization.
- **Impossible to analyze through classical means:** Traditional software and architectures are insufficient to handle the processing demands.

As the statistician Ronald H. Coase famously quipped, "If you torture data long enough, it will confess." The goal of Big Data techniques is to find ways to make this vast and complex data "confess" its valuable secrets.

### Core Concepts of a Big Data Approach

A complete and effective approach to handling Big Data must incorporate several key elements:
1.  **Efficient data collection techniques.**
2.  **Solid tools to handle massive amounts of data.**
3.  **Highly performant tools for data selection and querying.**
4.  **Efficient techniques for analysis and result selection.**

### The "3V" Model and Beyond

The most famous model for describing Big Data is the **"3V" model**, which has since been expanded.

- **Volume:** This refers to the enormous scale of data, measured in terabytes, petabytes, or even exabytes.
- **Velocity:** This describes the speed at which new data is generated and must be processed. This can range from batch processing (periodic) to near-real-time and real-time streams.
- **Variety:** This refers to the different forms of data. It can be **structured** (like in a traditional database table), **semi-structured** (like JSON or XML), or completely **unstructured** (like text, audio, and video from social media).

### Data Models: Classical vs. Big Data

The nature of Big Data necessitates a departure from classical data management approaches.

#### Classical Approach (Relational Databases)
- **Data Model:** Data is highly **structured**, typically using SQL (Structured Query Language).
- **Core Principle:** The guiding principle is **schema-on-write**. This means a rigid structure (schema) must be defined *before* any data can be written to the database.
- **Guarantees:** These systems are built on the **ACID** properties to ensure transaction reliability:
    - **Atomicity:** A transaction is an all-or-nothing operation; it either succeeds completely or fails entirely.
    - **Consistency:** A transaction brings the database from one valid state to another, ensuring data integrity rules are not violated.
    - **Isolation:** Concurrent transactions are self-contained and do not interfere with each other, as if they were executed sequentially.
    - **Durability:** Once a transaction is committed, its changes are permanent and survive any subsequent system failure.

#### Big Data Approach (NoSQL and Distributed Systems)
- **Data Model:** Systems are designed to handle **unstructured** or semi-structured data, using databases like MongoDB or Cassandra.
- **Core Principle:** The guiding principle is **schema-on-read**. This means raw data is stored as-is, and a structure or schema is applied only when the data is read or queried. This provides immense flexibility.
- **Trade-offs (CAP Theorem):** Distributed systems face a fundamental trade-off described by the **CAP Theorem**. It states that any networked shared-data system can provide at most **two** of the following three guarantees:
    - **Consistency (C):** Every read receives the most recent write or an error. All nodes have the same data at the same time.
    - **Availability (A):** Every request receives a (non-error) response, without the guarantee that it contains the most recent write.
    - **Partition Tolerance (P):** The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes.
    
    In a distributed system, network partitions are a fact of life, so **Partition Tolerance (P) is mandatory**. This means designers must choose between strong Consistency (a CP system) and high Availability (an AP system).

- **Guarantees (BASE):** As a result of this trade-off, many Big Data systems favor an approach known as **BASE**, which prioritizes availability over strict consistency.
    - **Basically Available:** The system guarantees that a response will be provided to any request, even if that response is a failure or contains inconsistent data.
    - **Soft State:** The state of the system may change over time, even without new input, as it works towards consistency. The data is treated more like a changing stream than a stable, fixed entity.
    - **Eventual Consistency:** The system will eventually become consistent once all write operations cease and synchronization tasks are completed across all nodes. It doesn't happen instantaneously, but it will happen.

---

## Big Data Examples

The principles of Big Data are not just theoretical; they are applied in numerous fields to solve real-world problems.

### Search Engines – Google
Google was a major catalyst for the development of Big Data techniques. Their need to index the entire web drove innovation.
- **Usage Scenarios:**
    - **Indexing and storing** massive collections of multimedia documents (web pages, images, videos).
    - **Result relevance analysis** that correlates a user's query with target keywords across this vast index to provide the most useful results.

### Security
In cybersecurity and finance, analyzing massive datasets in real-time is critical.
- **Fraud Detection:** Banking solutions analyze millions of transactions to identify patterns indicative of fraudulent activity.
- **Log Analysis:** Systems sift through huge volumes of system and network logs to detect atypical patterns that could signal a security breach or attack.

### Biomedical Field
The application of Big Data in healthcare is transforming patient care and medical research.
- **Decision Support Systems:** Assisting doctors in making a diagnosis or prescribing treatment by analyzing patient data against vast medical literature and case histories.
- **Health and Prescription Monitoring:** Tracking patient adherence to treatment plans and monitoring vital signs through wearable devices.
- **Early Disease Identification:** Identifying subtle markers of disease in early stages for more effective treatment.
- **Personal Health Monitoring:** Empowering individuals to better monitor and manage their own health status using data from apps and devices.

Essentially, Big Data techniques can be applied to any field that has previously used **Data Mining** and **Data Warehousing**, but on a much larger and more complex scale.

---

## Big Data Technologies

To implement these ideas, specific technologies were developed. Two of the most foundational are the Google File System and MapReduce.

### Google File System (GFS)
GFS is a distributed file system designed to run on large clusters of commodity hardware. It provides a fault-tolerant and scalable foundation for storing enormous datasets.
- **Architecture:** It is based on a **master-slave** architecture.
- **Slave Nodes:** These are **commodity computers** (inexpensive, standard machines).
    - Their primary role is to store multiple copies (replicas) of data blocks for redundancy and fault tolerance.
    - They also propagate data writing and replication instructions received from the master.
- **Master Node:** This is the single **coordinating node**.
    - It maintains all the file system metadata, including a complete map of which file partitions (chunks) are stored on which physical slave nodes.
    - It controls all `read-write` access to the files, directing clients to the appropriate slave nodes.
- **Usage:** GFS is the underlying storage system for Google's **BigTable**, a distributed storage system for managing structured data.

### MapReduce and Hadoop
While GFS solves the storage problem, **MapReduce** solves the processing problem.

#### MapReduce
MapReduce is a programming model and framework for processing and generating large datasets in a parallel, distributed manner.
- **Purpose:** It provides a development pattern for applications that target large volumes of data.
- **Data Representation:** It employs a **key-value** data representation, similar to a dictionary or hash map.
- **Core Features:** It is inherently **distributed, scalable, and fault-tolerant**. The framework handles the complexities of parallel execution, data distribution, and hardware failures.
- **Structure:** It uses a **stage-based structure** (the Map and Reduce phases), which makes complex processing tasks easier to manage.

#### Hadoop
Hadoop is the most popular **open-source implementation** of the MapReduce framework. It provides an entire ecosystem for big data storage and processing.
- **Core Components:**
    - **Hadoop Common Package:** Provides the common utilities and libraries that support the other Hadoop modules.
    - **HDFS (Hadoop Distributed File System):** An open-source implementation inspired by the Google File System (GFS). It provides the scalable and reliable storage layer.
    - **MapReduce Engine:** The implementation of the MapReduce programming model for large-scale data processing.

---

## MapReduce – Basics

Let's dive deeper into the MapReduce model itself.

### The MapReduce Model
MapReduce is an application development technique designed to abstract away the complexities of distributed computing.

- **Generics:**
    - It is designed specifically for **large-volume data processing**.
    - All data, both intermediate and final, is presented as **key/value pairs**. While powerful, some consider this a limitation.
    - Its API hides cumbersome parallelization details while providing **fault tolerance and load balancing**.
    - Problems must be expressed in terms of **data selection (map)** and **result composition (reduce)**.
    - The underlying architecture follows a **coordinator/worker** pattern (often called master/slave).

### The Stages of MapReduce

The process is divided into distinct stages. At a high level, there are two main phases.

1.  **Map Stage:** This is a *divide-et-impera* (divide and conquer) like stage.
    - The **coordinator** node takes the input data, divides it into smaller sub-problems or splits, and distributes these splits to the available **worker** nodes.
    - It's important to note that the number of data splits is not necessarily related to the number of workers; a single worker might be assigned multiple splits to process.
    - The job of each worker in the map stage is to process its assigned data split and emit intermediate key/value pairs.

2.  **Reduction Stage:**
    - The intermediate data from the map stage is collected and arranged into disjoint sets based on the key.
    - This data is then assigned to (new) workers that **compose or operate on the values** associated with each key to produce the final result.
    - Similar to the map phase, there is no direct relationship between the amount of data and the number of reducer workers. A single worker may be assigned multiple reduction tasks.

### A More Refined View of the Stages
Michael Kleber of Google further refined the model into a five-stage process that gives a clearer picture of the data flow:
1.  **Pre-processing:** An initial step to prepare the raw data for the mapping function. This might involve cleaning, parsing, or filtering.
2.  **Map:** Each worker applies the `map` function to its portion of the data to select interesting information and transform it into intermediate `<key, value>` pairs.
3.  **Shuffle and Sort:** This is the critical stage between map and reduce. The framework automatically collects all the intermediate pairs from all mappers, sorts them by key, and groups all values associated with the same key together. This ensures that all data for a single key goes to a single reducer, optimizing the reduction stage.
4.  **Reduction:** A worker applies the `reduce` function to each key and its associated list of values to produce the final output, which is often a smaller set of `<key, value>` pairs.
5.  **Store Result:** The output from the reduction phase is written to the distributed file system.

### Example Problems Solved with MapReduce

To make this concrete, let's look at a few classic problems.

#### Word Counting
The "Hello World" of MapReduce. The goal is to count the occurrences of each word in a large collection of documents.
- **`map(key, value)`:** Takes a document as input. For each `word` in the document, it emits a pair `<word, 1>`.
- **`reduce(key, values)`:** The key is a `word`, and `values` is a list of 1s (one for each time the word appeared). The function simply sums this list to get the total count `occ_counter` and emits the final pair `<word, occ_counter>`.

#### Back-link Identification
The goal is to find all pages that link to a given target page.
- **`map(key, value)`:** Takes a web page as input. For each `referrerURL` found on the page that links to a `targetURL`, it emits `<targetURL, referrerURL>`.
- **`reduce(key, values)`:** The key is a `targetURL`, and `values` is the list of all `referrerURLs` that link to it. The function simply collects these into a list and emits `<targetURL, list(referrerURLs)>`.

#### Inverted Indexing
A core task for search engines. The goal is to create an index mapping each word to a list of documents in which it appears.
- **`map(key, value)`:** Takes a document (with a `docID`) as input. For each `word` in the document, it emits `<word, docID>`.
- **`reduce(key, values)`:** The key is a `word`, and `values` is a list of all `docIDs` where that word was found. The function collects these into a list and emits `<word, list(docIDs)>`.



# Lecture 2 - The MapReduce Model

---

## Contents
- Definitions and Basic Concepts
- Execution Overview
- Open Discussion: Designing MapReduce Jobs

---

## Definitions and Basic Concepts

In our previous lecture, we introduced MapReduce as a foundational technology for Big Data processing. Now, we will delve deeper into the model itself, exploring its properties, architecture, and execution flow.

### Formal Definitions and Characteristics

MapReduce can be formally defined in a couple of ways that capture its essence:

-   It is a **programming model** and associated implementation for processing and generating large datasets, applicable to a wide variety of real-world tasks.
-   It is a **programming paradigm** that enables massive scalability across hundreds or thousands of servers, particularly within a Hadoop cluster.

The core characteristics of this model are:

-   It operates by processing a set of **key/value input pairs** to generate a set of **key/value output pairs**. This consistent data structure is fundamental to its operation.
-   It provides a powerful abstraction where the user only needs to focus on writing the logic for the **`map`** and the **`reduce`** functions. The framework handles all the complex underlying details.

### Ideal Properties

Ideally, a MapReduce framework should provide several key features automatically, freeing the developer from dealing with the complexities of distributed systems:

-   **Automatic Parallelization:** The framework should automatically distribute the data and execute the processing in parallel across the cluster.
-   **Good Load Balancing:** It should ensure that tasks are distributed evenly among the worker nodes to maximize efficiency and minimize idle time.
-   **Network and Disk Transfer Optimization:** It should handle the movement of data across the network and between memory and disk as efficiently as possible.
-   **Fault Tolerance and Robustness:** It must gracefully handle machine failures without losing data or halting the entire job.
-   **Open-Source Benefits:** Since many implementations (like Hadoop) are open-source, improvements made to the core library by the community should benefit all users.

### Core Properties of MapReduce

The design of MapReduce is guided by four main properties that make it so effective:

-   **Simplicity and Accessibility:** The programming model is centered around the core `map` and `reduce` methods. This simplicity allows developers to write code in a variety of popular languages, including **Java, C++, or Python**, without needing to be experts in distributed systems.
-   **Flexibility:** The model is adaptable and can process data from multiple sources and in multiple formats or representations.
-   **Speed:** Due to its intrinsic parallelism and design for distributed execution, MapReduce can process huge amounts of data very quickly.
-   **Scalability:** It is designed to scale horizontally. A well-designed MapReduce system can process petabytes of data, with performance scaling linearly as more machines are added to the cluster. The ultimate performance, of course, depends on the underlying tools (like the distributed file system) and their quality.

### The Stages of MapReduce
As we've seen, the process can be broken down into five logical stages, a refinement proposed by Michael Kleber of Google:

1.  **Pre-processing:** Prepare the raw input data for the mapping function.
2.  **Map:** Select interesting data and transform it into intermediate key/value pairs.
3.  **Shuffle and Sort:** Automatically reorganize the intermediate data, grouping all values for the same key together to prepare for the reduction phase.
4.  **Reduction:** Process the grouped data to determine the final result.
5.  **Store Result:** Write the final output to a persistent storage system.

---

## Execution Overview

Now let's examine how a MapReduce job is actually executed within a cluster.

### The Master/Worker Architecture

MapReduce operates on a **Master/Worker model**. This architecture consists of two types of nodes:

-   **Master Node:** A single, special node that acts as the coordinator for the entire job. It "decides" on crucial matters like:
    -   Data partitioning (how the input data is split up).
    -   Phase synchronization (managing the transition from the map to the reduce phase).
    -   Handling worker failures.
    -   It assigns **tasks** to worker nodes and monitors their execution. A **task** is defined as a partition of data combined with a computational instruction set (i.e., the `map` or `reduce` function to run on that data).

-   **Multiple Worker Nodes:** These are the machines that do the actual data processing. They follow the tasks assigned to them by the master and report their status and results. The rest of the nodes in the cluster must be able to access the master's data and the intermediate results produced during the job.

### The Basic Execution Model

The general flow of a MapReduce job is as follows:
1.  A user submits a program to the **Master** node. The master forks the program and takes control of the job execution.
2.  The Master assigns **map tasks** to available worker nodes and **reduce tasks** to others.
3.  **Map Phase:** Map workers read their assigned input splits (partitions of the original data).
4.  Each map worker executes the user-defined `map` function, producing intermediate key/value pairs which are written to local storage.
5.  **Reduce Phase:** Reduce workers remotely read the intermediate data from the map workers' local disks. The framework ensures that each reduce worker gets all the values for a specific key or set of keys.
6.  Each reduce worker executes the user-defined `reduce` function on its data and writes the final output to a file in the distributed file system.

### Details on the Master Node

The Master node orchestrates the entire process.

-   **Task Partitioning:**
    -   It first splits the input data into **M map tasks**. A common recommended size for each task is between 16MB and 64MB.
    -   It also partitions the intermediate key space into **R reduce tasks**, which determines the number of final output files.
    -   Tasks are assigned to workers **dynamically**.

-   **Why have more tasks than workers?** It is crucial that the number of tasks (`M` and `R`) is not tied to the number of physical worker machines. In fact, `M` and `R` should be much larger than the number of workers for several reasons:
    -   **Fault Recovery:** If a worker fails, its small task can be quickly reassigned to another worker, minimizing lost work.
    -   **Dynamic Load Balancing:** By having a large pool of small tasks, the master can assign new tasks to workers as soon as they finish their previous ones. This prevents a situation where a "straggler" (a single slow machine) holds up the entire job.
    -   **Pipelining:** It can facilitate pipelining between the map and reduce phases, where reducers can start pulling data as soon as mappers begin to complete.

-   **Phase Management:**
    -   **Map Phase:** The master assigns map tasks to free workers, trying to leverage **data locality** (i.e., assigning a task to a worker that already has the data on its local disk) to minimize network traffic. It continuously monitors the execution of these workers.
    -   **Reduce Phase:** The master assigns reduce tasks to free workers. It uses user configuration to determine how the intermediate keys should be partitioned among the reducers.

-   **Key Data Structures:** The master maintains several key data structures to manage the job:
    -   **Task State:** For every map and reduce task, it stores the state: `idle`, `in-progress`, or `completed`. This is essential for recovering from failures.
    -   **Worker Identity:** It keeps information on all worker nodes and the physical machines they are running on.
    -   It also tracks the locations of the **intermediate files** produced by the map tasks. This information is updated as each map task is finalized and is incrementally pushed to the reduce workers.

### Details on the Worker Nodes

#### Map Worker Steps
A map worker executes the following sequence of steps:
1.  Reads the assigned input partition.
2.  Applies the user-defined `map` function to the input, generating intermediate `<key, value>` pairs.
3.  Partitions its intermediate results into `R` splits (one for each reducer) using a partitioning function (e.g., `hash(key) mod R`).
4.  Writes these results to local storage. This location must be accessible to both the master and the reduce workers. While direct local storage is the classic approach, modern systems often recommend using a distributed file system as the communication medium.
5.  Notifies the master node upon completion of its task.

#### Reduce Worker Steps
A reduce worker executes the following sequence:
1.  Reads the assigned `reduce` key(s) and their associated intermediate values from the map workers.
2.  **Sorts** the intermediate results by key. This step is crucial for grouping all values for a single key together. *(Note: This sorting is often handled by the framework during the "shuffle and sort" phase, so the reducer itself might not need to perform it).*
3.  Applies the user-defined `reduce` function to the key and its list of values.
4.  Stores the final output file(s) in the distributed file system.

> **Important Note:** The `map` and `reduce` functions should be designed to be as **atomic** as possible (having a single, clear responsibility). Furthermore, it is highly desirable for them to be **idempotent**. An idempotent function is one that can be run multiple times on the same input and will always produce the same output. This is critical for fault tolerance, as it allows the framework to safely re-execute a failed task without causing incorrect results.

### Fault Tolerance and Load Balancing

#### Worker Failures
The framework is designed to be resilient to worker failures.
-   The **Master node** periodically sends **heartbeat** messages to each worker.
-   If a **map worker** fails to respond after a certain amount of time, the master marks all `in-progress` tasks assigned to that worker as `idle`. These tasks are then rescheduled on other workers.
    -   Crucially, if the intermediate results were stored only on the failed worker's local disk, any `completed` map tasks from that worker must also be reset to `idle` and re-executed, as their output is now lost.
-   If a **reduce worker** fails, only its `in-progress` tasks are reset to `idle` and reassigned. The output of a reduce task is written to a global, reliable file system, so completed tasks are safe.

#### Master Failures
Handling master failures is more complex but possible.
-   The **Master node's state** (task status, worker locations, etc.) can be periodically checkpointed and saved to the persistent file system.
-   If the master fails, a **new master node** can be "rebooted." It reads the last saved state and resumes the job from where it left off.

#### Load Balancing
In MapReduce, load balancing is not explicitly implemented with complex algorithms but is rather a natural outcome of the design.
-   It is achieved through the same means as fault tolerance: by breaking the job into many small tasks.
-   The Master dynamically assigns tasks from this large pool to the next available worker. This ensures that faster workers process more tasks and no worker sits idle while others are still working. The atomicity and granularity of the tasks are key to this "simulated" but effective load balancing.

---

## Open Discussion: Designing MapReduce Jobs

To solidify these concepts, consider how you would design the map and reduce phases for the following common problems:

-   **Word Counting:**
    -   `map` output: `<word, 1>`
    -   `reduce` output: `<word, total_count>`

-   **Back-link Identification:**
    -   `map` output: `<targetURL, referrerURL>`
    -   `reduce` output: `<targetURL, list(referrerURLs)>`

-   **Inverted Indexing:**
    -   `map` output: `<word, docID>`
    -   `reduce` output: `<word, list(docIDs)>`

**NOTE:** The examples provided primarily show the *output* for each phase. A critical part of the design process is reasoning about the **key selection**. Why have these specific keys been chosen? The choice of the intermediate key is fundamental, as it dictates how the data is grouped and sent to the reducers, ultimately determining whether the problem can be solved effectively.



# Lecture 3 - Text Indexing

---

## Contents
- Introduction to Text Indexing
- The Indexing Process
- Index Classes: Direct and Inverted
- Indexing Examples
- Sequential Indexing Algorithms
- Classic Parallelization Techniques
- Indexing using MapReduce

---

## Introduction to Text Indexing

Text indexing is a fundamental process in information retrieval and lies at the heart of how search engines work. It is a perfect real-world application to illustrate the power and design of the MapReduce model.

### Definitions
- **Indexing:** The process of organizing or reorganizing data to support fast and accurate searching.
- **The Indexer:** The computational process—a program or collection of programs—that performs the indexing.
- **WEB Indexing:** The specific application of indexing techniques to content available on the World Wide Web, including documents, media items, and more.

While web content is varied, the techniques used by search engines are primarily focused on **text indexing**. We study these techniques because they represent a core, native model for MapReduce, and the basic concepts are generalizable to other data types, like media files.

### Generic Search Engine Architecture

The role of the indexer becomes clear when we look at a generic search engine architecture.
1.  A **Web Crawler** (or spider) systematically browses the World Wide Web, discovering and fetching pages.
2.  These fetched documents are then passed to the **Indexer**.
3.  The **Indexer** processes these documents and creates highly optimized data structures called **Indexes**. These are often stored in massive databases.
4.  When a **User** submits a query through the search bar, the search engine does not scan the web in real-time. Instead, it queries these pre-built indexes to find relevant documents almost instantaneously.

### The Basic Task of Indexing

The primary purpose of indexing is to process a set of documents (like web pages) in such a way that one can quickly identify which documents contain a given **token** (a word or term) or a set of tokens.

-   **Input:** A target collection of documents.
-   **Output:** Search-optimized index data structures.
-   **Main Data Structure:** The output is typically a **dictionary-like representation**, where a `key` (e.g., a word) maps to a set of `values` (e.g., the documents containing that word).

> **Note:** It is crucial to design the indexing process with the search criteria in mind. The output of the indexer—the index itself—becomes a primary input for any given search function. The structure of the index directly dictates the speed and capabilities of the search.

---

## The Indexing Process

Creating an index is a multi-stage process that transforms raw documents into a structured, searchable format.

### Output Format

The final product of the indexer consists of two main parts:

-   **Index Dictionary:** A data set that includes all the unique indexing keys (tokens) identified in the entire document collection. It also contains "pointers" that indicate the actual location of the data associated with each key/token.
-   **Index Files:** The data sets that store the dictionary-style associations, linking the index keys/tokens to their corresponding values (e.g., document lists).

### Main Stages

The process can be broadly divided into two main stages:

1.  **Pre-processing:** Cleaning and preparing the raw text.
    -   Identifying meaningful tokens.
    -   Token processing (normalization).
2.  **Building the actual index:** Assembling the processed tokens into the final index structure.

### Pre-processing

Pre-processing is the critical first step of preparing a given set of data before the main analytical or indexing step. For text indexing, this involves altering text documents to eliminate or transform words and characters that could have a negative impact on the final index.

Several key issues must be addressed:
-   What words are considered **meaningful**? (e.g., should we index "the", "a", "is"?)
-   How should **punctuation** be handled? (e.g., is "e-mail" one token or two?)
-   Are **numerical values** significant for searching?
-   Are **phrases** better suited for indexing than simple, isolated words?
-   Should words be stored in their **canonical/base form**? (e.g., should "run", "running", and "ran" all be treated as the same concept?)

#### Token Selection

This stage involves extracting the basic units (tokens) from the text. Using an HTML document as an example, a simple process would be:

1.  **Prune HTML tags and attributes:** Most text within HTML tags is for formatting and is not rendered to the user, so it can be discarded. A possible exception is the `alt` attribute of an `<img>` tag, which contains descriptive text.
2.  **Split the text into tokens (words):**
    -   A **word** is typically defined as a sequence of alphabetic characters, ignoring punctuation and other separators.
    -   This step also involves pruning **stopwords**—common words like "a", "an", "the", "in"—which add little semantic value for searching. These are usually removed based on a predefined list.

#### Token Processing: Stemming and Lemmatization

The main goal of this stage is to determine the **canonical** or **base form** (the *lemma*) of words. This ensures that different forms of a word are treated as equivalent. This is a challenging, language-dependent task with two common techniques:

-   **Stemming:** A fast, heuristic-based process that chops off the ends of words to find a common "stem". It is often inaccurate and prone to errors because its simple rules have many exceptions. Common algorithms include the **Porter**, **Lovins**, and **Paice/Husk** stemmers.
-   **Lemmatization:** A more advanced technique that uses vocabulary and morphological analysis to return the base or dictionary form of a word (the lemma). It is far more accurate than stemming but is also much more time-consuming.

---

## Index Classes: Direct and Inverted

Once we have our processed tokens, we can build the index. There are two primary classes of indexes.

### Direct Indexing (Horizontal Indexing)

-   **Definition:** A direct index stores tokens based on the **document they belong to**. The primary sorting key is the document identifier (`docID`).
-   **Generic Format:** `<docID: {termIDx | termIDx is a token in docID}>`
    -   `docID`: A unique document identifier.
    -   `termIDx`: An identifier for a token belonging to that document.
-   **Utility:** Direct indexes are not typically used for direct searching. Instead, they are an intermediate step used to **build inverted indexes** and can be utilized for implementing more sophisticated relevance criteria (e.g., analyzing the properties of a specific document).

#### Sub-classes of Direct Indexes
-   **Boolean:** The list for each `docID` includes only the distinct tokens, without any supplemental data.
-   **Quantitative:** The list includes the **number of occurrences** for each token within the document.
-   **Positional:** The list includes the number of occurrences **and** the **relative positions** of the tokens within the document.

### Inverted Indexing (Vertical Indexing / Postings)

-   **Definition:** An inverted index stores documents based on the **tokens they include**. The primary sorting key is the token identifier (`termID`). This is the reverse, or "inversion," of a direct index.
-   **Generic Format:** `<termID: {docIDy | termID is in docIDy}>`
    -   `termID`: A unique token identifier.
    -   `docIDy`: An identifier for a document that contains the token.
-   **Utility:** This is the most important concept. **Inverted indexes are the core components of every single search engine!** They allow for extremely fast lookups of which documents contain a given search term.

#### Sub-classes of Inverted Indexes
-   **Boolean:** The list for each `termID` (the *postings list*) includes only the distinct document identifiers, without any supplemental data. It is mainly used for simple boolean search functions (e.g., find documents containing "data" AND "mining").
-   **Quantitative:** The postings list also stores the **number of occurrences** of the token within each respective document. This is essential for ranking algorithms like TF-IDF.
-   **Bi-word Index:** A variation where the indexing keys are **pairs of consecutive tokens**. The generic format is `<(termIDᵢ, termIDᵢ₊₁): {docIDy}>`. This helps solve a small subset of search cases (phrase searches) but can add significant processing complexity.
-   **Positional:** This is the most advanced form. The postings list for a token includes not only the document ID but also the **number of occurrences** and the **relative positions** of the token within that document. This is critical for proximity and phrase searching (e.g., finding "data analysis" as an exact phrase). The format looks like:
    `<termID: { (docIDy, count, {position₁,...}) , ... }>`

---

## Indexing Examples

Let's consider a small collection of documents to see how these indexes look in practice.

**Documents:**
-   **Doc1:** Data mining is a new data analysis technique.
-   **Doc2:** Data mining techniques may yield new information.
-   **Doc3:** Data are collected through dedicated techniques.

**Vocabulary (after pre-processing):**
`data`, `mining`, `technique`, `new`, `analysis`, `information`

| **Direct Boolean Index**                                     |
| :----------------------------------------------------------- |
| **Doc1:** `{analysis, data, mining, new, technique}`         |
| **Doc2:** `{data, information, mining, new, technique}`      |
| **Doc3:** `{data, technique}`                                |

| **Direct Quantitative Index**                                                          |
| :------------------------------------------------------------------------------------- |
| **Doc1:** `{(analysis, 1), (data, 2), (mining, 1), (new, 1), (technique, 1)}`           |
| **Doc2:** `{(data, 1), (information, 1), (mining, 1), (new, 1), (technique, 1)}` |
| **Doc3:** `{(data, 1), (technique, 1)}`                                                |

| **Quantitative Inverted Index**                                   |
| :------------ | :------------------------------------------------ |
| **analysis**  | `{(Doc1, 1)}`                                     |
| **data**      | `{(Doc1, 2), (Doc2, 1), (Doc3, 1)}`               |
| **information**| `{(Doc2, 1)}`                                     |
| **mining**    | `{(Doc1, 1), (Doc2, 1)}`                          |
| **new**       | `{(Doc1, 1), (Doc2, 1)}`                          |
| **technique** | `{(Doc1, 1), (Doc2, 1), (Doc3, 1)}`               |

| **Positional Inverted Index**                                                     |
| :------------ | :---------------------------------------------------------------- |
| **analysis**  | `{(Doc1, 1: <5>)}`                                                |
| **data**      | `{(Doc1, 2: <1, 6>), (Doc2, 1: <1>), (Doc3, 1: <1>)}`              |
| **information**| `{(Doc2, 1: <4>)}`                                                |
| **mining**    | `{(Doc1, 1: <2>), (Doc2, 1: <2>)}`                                |
| **new**       | `{(Doc1, 1: <4>), (Doc2, 1: <3>)}`                                |
| **technique** | `{(Doc1, 1: <7>), (Doc2, 1: <3>), (Doc3, 1: <5>)}`              |


---

## Sequential Indexing Algorithms

When the document collection is too large to fit in memory, we need algorithms that can build an index by processing the data in chunks.

### Blocked Sort-Based Indexing (BSBI)
The main idea of BSBI is to divide the document collection into fixed-size blocks that can fit in memory.
1.  **Process Block-by-Block:**
    -   Parse the next block of documents to identify `(termID, docID)` pairs.
    -   Within that block, sort these pairs, using `termID` as the primary key and `docID` as the secondary key.
    -   Compute a local inverted index for this block and write it to disk.
2.  **Merge Blocks:** After all blocks have been processed, perform a multi-way merge of all the on-disk index files to create the final, unified inverted index.

### Single-Pass In-Memory Indexing (SPIMI)
SPIMI offers an optimization over BSBI. Its main idea is to dynamically build the inverted index in a single pass over the data without creating the full `termID-docID` list first.
1.  It processes a stream of tokens `(term, docID)`.
2.  It builds an in-memory dictionary with a postings list for each term.
3.  When memory becomes full, it sorts the terms in the current dictionary, writes that block to disk, and clears the memory to start a new block.
4.  Like BSBI, it finishes with a step to merge the on-disk blocks.

A key particularity is that SPIMI often uses the term string itself as the key in the dictionary, eliminating the need for a separate mapping from words to numeric `termID`s, which can lead to better memory management.

---

## Classic Parallelization Techniques

To handle web-scale document collections, we must parallelize the indexing process across many machines. The goal is to support larger collections and faster searching while minimizing communication overhead. There are two common approaches.

### Term-Based Partitioning (Global Indexing)
-   **Core Idea:** Divide the working **vocabulary** (the set of all terms) among the computing nodes.
-   **Process:** Each node is responsible for building the complete index entries for its assigned subset of terms. For example, Node 1 handles all terms from 'a' to 'f', Node 2 handles 'g' to 'p', and so on. To do this, every node must have access to every document.
-   **Advantage:** This is very well-suited for **concurrent searches**. A query for a term is sent directly to the specific node responsible for that term.
-   **Drawback:** The partitioning function can be complex to design (to balance the load), and maintaining the distributed index can be challenging.

### Document-Based Partitioning (Local Indexing)
-   **Core Idea:** Divide the **document collection** into even splits.
-   **Process:** Each computing node is assigned a split of the documents. It then builds a complete, but **local**, inverted index for *only* the documents in its split.
-   **Advantage:** The indexing process itself requires very little communication between nodes, as each one can operate independently on its local data.
-   **Drawback:** The main drawback appears at **query time**. A global search query must be sent to **every single node**. The results from all nodes must then be collected and merged, which relies on heavy communication.

---

## Indexing using MapReduce

MapReduce naturally implements a parallel indexing strategy. The standard approach for building an inverted index with MapReduce is a form of **Term-Based Partitioning**.

### Inverted Index Example
-   **Map Phase (Parser):**
    -   The `map` function acts as the **parser**. It takes a split of documents as input.
    -   It processes each document, emitting intermediate key/value pairs of `<term, docID>`.
    -   These intermediate results are written to local *segment files*. The MapReduce framework's partitioning function ensures that all pairs for a given range of terms are sent to a specific reducer.

-   **Reduce Phase (Inverter):**
    -   The `reduce` function acts as the **inverter**.
    -   Each reducer is responsible for a specific partition of the term space (e.g., reducer 1 for 'a-f', reducer 2 for 'g-p').
    -   It receives a key (a term) and a list of all `docID`s associated with that term from the map phase.
    -   It then aggregates these values to create the final postings list for that term and writes it to the final output file.


# Lecture 5 - Graph Analysis using MapReduce

---

## Contents
- The Shortest Path Problem
    - Graph Definitions and Representations
    - Sequential Algorithms for SSSP
- MapReduce Approaches for Shortest Path
    - Challenges and Core Ideas
    - Parallel BFS with MapReduce
    - Adapting for Dijkstra's Algorithm
- The PageRank Algorithm
    - Introduction and Core Concepts
    - The PageRank Formula and Sequential Algorithm
    - A MapReduce Approach for PageRank

---

## The Shortest Path Problem

Many real-world problems can be modeled as graphs, from social networks and web links to transportation systems. Analyzing these massive graphs is a core task in Big Data. We'll begin by focusing on a classic graph problem: finding the shortest path.

### Graph Definitions and Representations

First, let's establish some fundamental definitions.

-   **Graph:** A graph is a set of items, called **vertices** (or nodes), connected by **edges**. Formally, a graph `G` is a pair `(V, E)`, where `V` is the set of vertices and `E` is the set of edges connecting pairs of vertices.
-   **Directed Graph (Digraph):** A graph whose edges are **ordered pairs** of vertices. An edge `(u, v)` has a specific direction from a **source vertex** `u` to a **destination vertex** `v`.
-   **Weighted Graph:** A graph where each edge is assigned a numerical **weight**, which can represent cost, distance, time, etc.
-   **Vertex Degree:**
    -   In a standard graph, the **degree** is the total number of edges connected to a vertex.
    -   In a digraph, the **in-degree** is the number of inbound edges, and the **out-degree** is the number of outbound edges. A node with an out-degree of 0 is sometimes called a **sink**.
-   **Graph Diameter:** The **diameter** of a graph is the **maximum shortest path distance** between any possible pair of vertices in the graph.
    -   For an **unweighted graph**, the distance is the number of edges on the shortest path.
    -   For a **weighted graph**, the distance is the sum of edge weights along the shortest path.

#### Common Graph Representations
There are two common ways to represent a graph in a computer:

1.  **Adjacency Matrix:** A square matrix where the entry `A[i][j]` is 1 if there is an edge from vertex `i` to vertex `j`, and 0 otherwise. For weighted graphs, the entry would be the edge weight.
2.  **Adjacency Lists:** For each vertex, we maintain a list of all the vertices it is connected to. This is often more space-efficient for sparse graphs (graphs with relatively few edges).

For example, for the following directed graph:
- **Node 1** connects to `2` and `5`.
- **Node 2** connects to `1` and `4`.
- **Node 3** connects to `2` and `4`.
- etc...

The **Adjacency List** representation would be:

1 : [2, 5]  
2 : [1, 4]  
3 : [2, 4]  
4 : [5, 6]  
5 : [ ]  
6 : [5]

This adjacency list representation is very common in parallel graph processing.

### Sequential Algorithms for SSSP

A fundamental graph problem is the **Single Source Shortest Path (SSSP)** problem: given a starting node, find the shortest path from that node to all other reachable nodes in the graph.

-   This is typically applied to directed graphs.
-   For unweighted graphs, we use **Breadth-First Search (BFS)**.
-   For weighted graphs with non-negative weights, we use **Dijkstra's algorithm**.

The basic idea in these algorithms is the same: iteratively select the next vertex in the "frontier" and update the status of its neighbors. However, they are **notoriously difficult to parallelize**. They rely on maintaining a global state of "visited" versus "un-visited" vertices, which is challenging to manage in a distributed memory system where no single machine has a complete view of the graph.

#### Dijkstra's Algorithm (Sequential)
Dijkstra's algorithm works by maintaining a set of unvisited nodes and, for every node, the current known shortest distance from the source.
1.  Initialize all distances to infinity, except for the source node which is 0.
2.  While there are unvisited nodes:
    a. Select the unvisited node `u` with the smallest known distance.
    b. Mark `u` as visited.
    c. For each neighbor `v` of `u`, calculate a new potential distance: `distance(u) + weight(u, v)`.
    d. If this new distance is shorter than the currently known distance to `v`, update `v`'s distance.
3.  The algorithm terminates when all reachable nodes have been visited.

---

## MapReduce Approaches for Shortest Path

### Problem Statement and Challenges

How can we adapt an iterative, state-dependent algorithm like Dijkstra to fit the stateless, parallel model of MapReduce?

-   **The Challenge:** A direct, "head-on" implementation of Dijkstra is impossible in a single MapReduce pass. The algorithm's strength lies in its greedy approach of always exploring from the globally closest node, a concept that doesn't exist when the graph is partitioned across many mappers.
-   **The Idea:** We can reframe the problem. Instead of a greedy search, we can use a **parallel Breadth-First Search (BFS)** approach. This is essentially a "brute force" method where we explore all paths of a certain length in parallel in each iteration. This **works**, but requires multiple MapReduce iterations.
-   **How many iterations?** In the worst case, the number of iterations required to find the longest shortest path is equal to the **diameter of the graph**.

### Parallel Breadth-First Search (BFS) with MapReduce

We can implement a parallel BFS to find the shortest path in an **unweighted** graph. This requires running the MapReduce job iteratively.

#### Data Modeling
We model the graph with the node ID as the key and a structure containing the node's information as the value.
-   **Key:** `NodeID`
-   **Value:** A custom structure, e.g., `(Distance, AdjacencyList)`
    -   `Distance`: The current shortest known distance from the source node.
    -   `AdjacencyList`: The list of neighbors for this node.

#### Iterative MapReduce Flow
The algorithm proceeds in iterations. In each iteration `i`, we are finding all nodes that are at a distance `i` from the source.

-   **Map Phase:** The mapper receives a node `n` with its current distance `d`. Its job is to explore one level deeper.
    1.  **Preserve Graph Structure:** The mapper first emits its own node structure `(n, (d, AdjacencyList))`. This is crucial because it passes the graph structure along to the reducer, ensuring it's not lost between iterations.
    2.  **Propagate Distances:** For each neighbor `m` in `n.AdjacencyList`, the mapper emits a new key-value pair: `(m, d + 1)`. This message tells node `m` that it can be reached from `n` with a new potential path length.

-   **Reduce Phase:** The reducer's job is to process all the incoming "messages" for a given node and find the new shortest path.
    1.  The reducer receives a key `n` and a list of values. This list will contain **one** complete node structure (from the graph preservation step) and potentially **multiple** new distance values (from its neighbors).
    2.  It finds the minimum distance `d_min` among all the incoming distance values.
    3.  It compares `d_min` with the node's current distance and updates it if the new path is shorter.
    4.  It emits the updated node structure `(n, (new_distance, AdjacencyList))` which becomes the input for the next MapReduce iteration.

The entire MapReduce job is re-run until an iteration completes with no changes to any node's distance, at which point the algorithm has converged.

### Adapting for Dijkstra's Algorithm

This iterative BFS model can be easily adapted to find the shortest path in a **weighted** graph (Dijkstra's).

-   The only significant change is in the **map phase**. Instead of emitting a distance of `d + 1`, the mapper emits `d + weight(n, m)`, where `weight(n, m)` is the weight of the edge from the current node `n` to its neighbor `m`.
-   The rest of the iterative process remains the same. The job is repeated until the distances stabilize.

---

## The PageRank Algorithm

The PageRank algorithm, developed by Larry Page and Sergey Brin in 1998, is another famous graph algorithm and was the core of Google's original search engine. It's a perfect example of an iterative algorithm that can be implemented with MapReduce.

### Introduction and Core Concepts

-   **Purpose:** To determine a weighted **relevance score** for every page on the web.
-   **The Core Idea:** PageRank uses a **voting schema**. A link from page A to page B is treated as a "vote of confidence" from A to B.
-   **Recursive Nature:** A page's PageRank score is not just a simple count of incoming links. It depends on the **PageRank scores of the pages that cast the votes**. A vote from a high-authority page (like Wikipedia) is worth more than a vote from an unknown blog.

### The PageRank Formula and Sequential Algorithm

The basic idea is that a page's PageRank is the sum of the PageRank scores of all pages that link to it. However, a page distributes its own PageRank score evenly among all of its outgoing links.

This leads to the foundational formula:
`PR(A) = Σ (PR(P) / LC(P))`
where the sum is over all pages `P` that link to `A`, and `LC(P)` is the total number of outgoing links from page `P`.

To account for a "random surfer" who might stop clicking links and jump to a random page, a **damping factor `d`** (usually 0.85) is introduced. The complete formula is:
`PR(A) = (1 - d) + d * Σ (PR(P) / LC(P))`

This is an iterative formula. The process starts by assigning an initial PageRank to all pages (e.g., `1/N`, where `N` is the total number of pages) and then repeatedly applies this formula across the entire graph until the PageRank values for all pages converge and stabilize.

### A MapReduce Approach for PageRank

Like our SSSP algorithm, PageRank is implemented by **iteratively running a MapReduce job**.

#### Data Modeling
-   **Key:** `PageID` (Node ID)
-   **Value:** A custom structure containing `(PageRank, AdjacencyList, NCount)`
    -   `PageRank`: The current PageRank score of the page.
    -   `AdjacencyList`: The list of pages that this page links to.
    -   `NCount`: The total number of outgoing links (the size of the adjacency list).

#### Map Phase
The map function is responsible for distributing a page's PageRank to its neighbors.
1.  It receives a node `n` with its current `PageRank` and its adjacency list of size `N.NCount`.
2.  **Preserve Graph Structure:** It first emits `(nid, N)` to pass the node's full structure (including its adjacency list) to the reducer.
3.  **Distribute PageRank:** It calculates the PageRank to distribute: `p = N.PageRank / N.NCount`.
4.  For each neighbor `m` in its `AdjacencyList`, it emits `(m, p)`, sending that portion of its PageRank to `m`.

#### Reduce Phase
The reduce function is responsible for collecting the distributed PageRank and calculating the new score for a page.
1.  It receives a page ID `nid` and a list of values. This list will contain one full node structure and many partial PageRank contributions.
2.  It initializes a variable `total_incoming_rank = 0`. It also extracts the `AdjacencyList` from the full node structure it received.
3.  It iterates through the list of values. If a value is a partial rank, it adds it to `total_incoming_rank`.
4.  After summing all contributions, it applies the full PageRank formula:
    `new_PageRank = (1 - d) + d * total_incoming_rank`
5.  It emits the final key-value pair for this iteration: `(nid, (new_PageRank, AdjacencyList, ...))`, which will be the input for the next iteration.

# Lecture 6 - Frequent Itemsets and Association Rules

---

## Contents
- Association Rule Mining
    - Definitions and Basic Concepts
    - The Apriori Algorithm
    - Parallel Approaches for Apriori
    - The FP-Growth Algorithm
- MapReduce Approaches for Frequent Itemset Mining

---

## Association Rule Mining

Beyond graph analysis, another major area of Big Data analytics is discovering hidden patterns and relationships within large datasets. This is the domain of **association rule mining**, a technique made famous by "market basket analysis," which seeks to find relationships between items that are frequently purchased together.

### Definitions and Basic Concepts

#### Problem Definition
The core problem is defined as follows: Given a set of all possible **items** `I` and a set of **transactions** `T` (where each transaction is a set of distinct items from `I`), the goal is to determine all interesting rules of the form:

`A ⇒ B`

This rule implies a conditional relationship where `A` and `B` are disjoint sets of items (`A, B ⊂ I` and `A ∩ B = ∅`). The inference should be read as: "What is the probability of finding the itemset `B` in a transaction if the itemset `A` is already present in that same transaction?"

#### Core Terminology
To understand the algorithms, we must first define some key terms:

-   **Itemset:** A collection or set of one or more distinct items. A `k-itemset` is an itemset containing exactly `k` items.
-   **Support:** The **support** of an itemset is its frequency in the dataset, defined as the **total number of transactions** that contain the itemset. It's an absolute count.
-   **Frequent Itemset:** An itemset is considered **frequent** if its support is greater than or equal to a user-defined threshold called the **minimum support limit**.
-   **Maximal Itemset:** An itemset is **maximal** if it is frequent, and no superset of it is also frequent. It represents the longest frequent pattern.
-   **Association Rule:** The rule `A ⇒ B` consists of two parts:
    -   **Support:** The support of the rule is the support of the combined itemset `A ∪ B`. This tells us how frequently `A` and `B` appear together.
    -   **Confidence:** The confidence of the rule is the conditional probability `P(B|A)`. It measures the "trust" or strength of the rule and is calculated as:
        `confidence(A ⇒ B) = support(A ∪ B) / support(A)`

#### Stages of Association Rule Mining
The process is universally broken down into two main stages:

1.  **Find Frequent Itemsets:** Scan the entire transaction set `T` to identify all itemsets that meet the minimum support threshold. This is the most computationally expensive part of the process and will be our focus.
2.  **Generate Association Rules:** Use the frequent itemsets found in stage 1 to generate all possible association rules that meet a **minimum confidence** threshold.

For example, if `{Beer, Diapers, Nuts}` is a frequent itemset, we can test rules like `{Beer, Diapers} ⇒ {Nuts}` by calculating its confidence. Stage 1 is often sufficient for many analyses (like access log analysis), as knowing which items co-occur frequently is valuable in itself.

### The Apriori Algorithm

The Apriori algorithm, published by Agrawal and Srikanta in 1994, is the classic sequential algorithm for finding frequent itemsets.

-   **Core Idea:** It uses a bottom-up, level-wise search. It determines frequent `k`-itemsets by using the frequent `(k-1)`-itemsets from the previous level.
-   **Characteristics:** It requires `k` full scans of the database, where `k` is the size of the largest frequent itemset.

#### The Apriori Principle
The algorithm's efficiency comes from a powerful property of support, known as the **Apriori Rule** or the *anti-monotone property*:

> **If an itemset is frequent, then all of its subsets must also be frequent.**

The logical inverse of this rule is used for pruning the search space:

> **If an itemset is *not* frequent, then none of its supersets can be frequent.**

This means if we find that `{Beer}` is not a frequent item, we can immediately prune away all larger itemsets containing beer, such as `{Beer, Diapers}` and `{Beer, Nuts}`, without ever having to count their support.

#### The Sequential Algorithm
Apriori proceeds in passes:
1.  **Pass 1:** Scan the database to find the support of all 1-itemsets. The set of frequent 1-itemsets is `L₁`.
2.  **Pass k (for k ≥ 2):**
    a. **Candidate Generation:** Generate a set of candidate `k`-itemsets, `Cₖ`, from the frequent `(k-1)`-itemsets, `Lₖ₋₁`. This is done via a "self-join" of `Lₖ₋₁`, followed by a pruning step where any candidate with an infrequent `(k-1)`-subset is removed.
    b. **Support Counting:** Scan the database. For each transaction, determine which candidates from `Cₖ` it contains and increment their counts.
    c. **Pruning:** The new set of frequent `k`-itemsets, `Lₖ`, is formed by keeping only the candidates from `Cₖ` that meet the minimum support.
3.  **Termination:** The algorithm stops when a pass generates no new frequent itemsets (`Lₖ` is empty). The final answer is the union of all `Lₖ` sets found.

### Parallel Approaches for Apriori

To scale Apriori to large datasets, several parallel approaches have been developed.

#### Count Distribution
-   **Idea:** The database is split among processing nodes. However, **each node holds the full list** of candidate itemsets for the current pass.
-   **Process:** Each node computes the *local* support for all candidates based on its local data partition. A `reduce`-like step then aggregates these local counts to determine the *global* support for each candidate.
-   **Drawback:** This approach involves **redundant computation**. Every processing node independently generates the exact same set of candidate itemsets, which is inefficient.

#### Data Distribution
-   **Idea:** Both the database and the **candidate computation are distributed** among the nodes.
-   **Process:** Each node is responsible for a subset of candidates. It computes local support for its candidates. Then, a massive communication step is required where nodes exchange database partitions or support counts to determine the global support for their respective candidates.
-   **Drawback:** This approach can be crippled by the **large volume of data transfer** required between nodes, proving inefficient due to communication overhead.

### The FP-Growth Algorithm

The FP-Growth algorithm, published by Han et al. in 2000, is a highly efficient alternative that avoids the most expensive part of Apriori: candidate generation.

-   **Core Idea:** It identifies frequent itemsets **without generating candidates**. It uses a compact prefix-tree data structure, called an **FP-tree**, to represent the database.
-   **Characteristics:** It requires only **two full scans** of the database, making it significantly faster than Apriori for many datasets.

#### Stages of FP-Growth
1.  **Database Preprocessing (First Scan):**
    -   Scan the database once to find the support of all 1-items and identify the frequent ones.
    -   Prune all infrequent items from every transaction.
    -   Re-organize the items within each transaction, sorting them in descending order of their frequency.
2.  **Initial FP-Tree Construction (Second Scan):**
    -   Build the initial FP-tree by scanning the preprocessed database. Each transaction is inserted as a path into the tree. Nodes on the path share prefixes, which makes the structure very compact.
    -   Each node stores an item and its local support count. A separate "header table" links all nodes for the same item, forming a chained list.
3.  **Recursive Mining (FP-Tree Projection):**
    -   The algorithm recursively mines the FP-tree to find frequent itemsets. It starts from the least frequent item in the header table and works its way up.
    -   For each item, it constructs a "conditional FP-tree" and mines it recursively. This "projection" process finds all frequent itemsets ending in that item, effectively building patterns from the bottom up without generating candidates.

---

## MapReduce Approaches for Frequent Itemset Mining

### Problem Statement and Potential Drawbacks
Adapting these algorithms, especially the iterative Apriori, to MapReduce presents challenges:

-   **Synchronization:** MapReduce stages are synchronous (`map` must finish before `reduce`). Apriori's model, `stageₖ = function(stageₖ₋₁)`, naturally requires multiple synchronized runs of a MapReduce job, one for each pass `k`.
-   **Candidate Generation:** The candidate generation and pruning steps of Apriori are difficult to distribute efficiently in MapReduce.
-   **Compatibility:** Some parallel models, like the *data distribution* approach for Apriori, are not a good fit for MapReduce due to their reliance on heavy inter-node communication.

### The MapReduce Model
There are two general strategies for implementing frequent itemset mining in MapReduce.

1.  **Multiple MapReduce Runs (Apriori-like):** The most common approach is to closely mimic the classic Apriori algorithm by executing `k` complete MapReduce jobs, where `k` is the size of the largest itemset. The output of job `k-1` (the frequent `(k-1)`-itemsets) is used to configure the candidate generation for job `k`.

2.  **Single MapReduce Run:** An alternative approach redefines the problem to fit MapReduce better.
    -   **Redefined Candidates:** Instead of a level-wise generation, a **candidate itemset** is defined as *any subset* of any transaction.
    -   **Advantage:** This allows all frequent itemsets to be determined in a **single full scan** of the database (one MapReduce job).
    -   **Drawback:** This approach generates a *considerably* larger number of candidate itemsets upfront, which can be a potential performance bottleneck in the map phase.

#### A Single-Pass MapReduce Algorithm
This powerful model works as follows:

-   **Map Stage:**
    -   **Input:** `<TransactionID, items_in_transaction>`
    -   **Process:** For each transaction, the mapper generates **all possible subsets (itemsets)** of the items in that transaction.
    -   **Output:** For each generated `itemset`, the mapper emits a key-value pair: `<itemset, 1>`.

-   **Shuffle & Sort / Combiner Stage:**
    -   **Process:** Before sending data to the reducers, a **Combiner** runs on each mapper node. It receives all pairs for a given `itemset` and sums the counts locally.
    -   **Output:** `<itemset, local_support>`
    -   This step is a critical optimization that significantly reduces the amount of data transferred over the network.

-   **Reduce Stage:**
    -   **Input:** `<itemset, list(local_support)>`
    -   **Process:** The reducer receives an `itemset` and a list of its local support counts from all the combiners where it appeared. It simply sums these values to get the final `global_support`.
    -   **Output:** If `global_support` is greater than or equal to the `min_support` threshold, the reducer emits the final result: `<itemset, global_support>`.

# Lecture 7 - Dimensionality Reduction

---

## Contents
- Data Dimensionality and the "Curse of Dimensionality"
- The Solution: Dimensionality Reduction
- Variation and Correlation
- Principal Component Analysis (PCA)
    - The Goal of PCA
    - The Steps of PCA
    - Case Study: Eigenfaces

---

## Data Dimensionality and the "Curse of Dimensionality"

Real-world data is rarely simple; it's almost never just a set of single scalar values. Instead, data items typically have multiple attributes or characteristics, which can be correlated or uncorrelated with each other.

### Data as a Multi-dimensional Space
These attributes can be considered **dimensions** of the data.
-   1 attribute (e.g., a person's height) = 1D data
-   2 attributes (e.g., a person's height and weight) = 2D data
-   `n` attributes = `n`-dimensional data

These dimensions collectively form an **n-dimensional space** where we can represent our data geometrically:
-   An **axis** of this space represents an attribute of the data.
-   A **position** on an axis represents a specific value of that attribute.
-   A single **point** in this space represents a complete data element (e.g., a single row in a table).
-   A **region** in this space may contain a cluster of similar data elements.
-   A **surface** (an `(n-1)`-dimensional boundary) can be used to separate data into distinct categories, as is common in machine learning classification.

### High-Dimensional Data
In many modern applications, we deal with **high-dimensional data**, where the number of dimensions is very large. In some cases, the number of dimensions can even exceed the number of data points (samples).

A classic example is a **DNA microarray**. These are used to simultaneously analyze thousands of genes from a relatively small number of samples. Each gene acts as a dimension, so a single sample might be a point in a space with 50,000 to 140,000 dimensions.

### The Problem: The "Curse of Dimensionality"
As we add more and more dimensions, the data becomes very difficult to work with. This phenomenon is known as the **"Curse of Dimensionality."**

-   **Increasing Sparsity:** As the number of dimensions increases, the volume of the data space grows exponentially. The same number of data points become spread out across this much larger volume. Consequently, the available data becomes increasingly **sparse**, meaning the points are rarer and farther apart from each other.
-   **Statistical Unreliability:** This sparsity severely affects the reliability of statistical analysis. The amount of data needed to achieve statistical significance grows exponentially with the number of dimensions. With sparse data, any statistical measurements become less meaningful.
-   **Problematic Searching and Clustering:** In high-dimensional space, our intuitive understanding of distance and grouping breaks down. Objects that might seem grouped in lower dimensions appear more unstructured and dissimilar from each other as more dimensions are added. This makes tasks like clustering and nearest-neighbor search very difficult.

---

## The Solution: Dimensionality Reduction
The solution to the "Curse of Dimensionality" is to reduce the number of dimensions we are working with. The goal is not to discard information arbitrarily, but to do so intelligently.

The key objectives are:
-   **Find important dimensions:** Identify the most useful attributes in the data.
-   **Find directions of most change:** Identify the axes along which the data has the most variation.
-   **Find and remove irrelevant attributes:** Identify characteristics that are not meaningful or are statistically irrelevant to the analysis.

### What Really Matters in Data?
In most real-world scenarios, the absolute raw data values are not as important as what we can learn from them. What usually matters more are the underlying patterns and relationships.

-   **Variation:** How does the data change? Which attributes change their values more or less than others?
-   **Correlation:** How do attributes change together? Do they tend to change in the same direction (positive correlation) or in opposing directions (negative correlation)? Does changing one attribute affect the variation of another?

This leads to the idea of extracting **features** from the data. Features are a reduced or compressed summary of certain representative aspects of the data. They contain the important information in far fewer values and in a lower-dimensional space than the original data itself.

For example, for a website, the absolute number of users at any given moment is less important than its **variation** (is the user base growing or shrinking?) and its **correlation** with other attributes (does the number of users decrease with user age? Is it independent of user gender?). If user count is found to be independent of gender, then "gender" might be considered a meaningless attribute for this analysis, and its dimension could be dropped.

---

## Variation and Correlation

To perform dimensionality reduction, we first need to formally measure variation and correlation.

-   **Variance:** A measure of the variability or spread of the data for a single attribute.
    -   `Var(x) = Σ(Xᵢ - X̄)² / N`
    -   High variance means the values are spread out; low variance means they are clustered close to the mean; zero variance means the values are constant.

-   **Covariance:** A measure of the extent to which corresponding elements from two attributes move in the same direction.
    -   `Covar(X, Y) = Σ(Xᵢ - X̄)(Yᵢ - Ȳ) / N`
    -   **Positive covariance:** As one attribute increases, the other tends to increase.
    -   **Negative covariance:** As one attribute increases, the other tends to decrease.
    -   **Zero covariance:** The attributes are uncorrelated; there is no predictable relationship between them.

### The Covariance Matrix
We can express the variance and covariance for all attributes in a dataset together in a single **covariance matrix**. For a dataset with `c` attributes, this is a `c x c` matrix where:
-   The elements on the **main diagonal** are the **variances** of each attribute.
-   The off-diagonal elements are the **covariances** between pairs of attributes.

For example, consider a dataset of student grades in three subjects (Math, Physics, Chemistry). The covariance matrix would be a 3x3 matrix. By analyzing it, we can learn:
-   Which subject's grades have the highest variance (most spread out).
-   Which subjects are positively correlated (e.g., students who do well in Math also tend to do well in Physics).
-   Which subjects are uncorrelated (e.g., performance in Physics has no relationship to performance in Chemistry).

---

## Principal Component Analysis (PCA)

**Principal Component Analysis (PCA)** is one of the most popular and powerful techniques for dimensionality reduction.

*(English language warning: it is **principal**, as in "main" or "primary", not "principle" as in "a rule".)*

-   **Goal:** PCA exploits data variation to find the most important directions in the data space.
-   **Method:** It projects the data from a high-dimensional space onto a lower-dimensional space while preserving as much of the original variance as possible.

### The Goal of PCA
For an n-dimensional dataset, PCA determines a new set of `k` (where `k < n`) orthogonal (perpendicular) vectors called **principal components**.

These vectors are ordered by the amount of variance they capture:
-   **The first principal component (PC1)** is the vector that points in the direction of the **largest variation** in the data.
-   **The second principal component (PC2)** is orthogonal to PC1 and points in the direction of the **second-largest variation**.
-   **The third principal component (PC3)** is orthogonal to both PC1 and PC2 and points in the direction of the third-largest variation, and so on.

By identifying these components, we can transform the data. The idea is that if the first few principal components capture most of the significant variation, the remaining dimensions can be discarded with minimal loss of information.

### The Steps of PCA

PCA is a well-defined process with four main steps.

#### 1. Calculate the Covariance Matrix of the Data
The first step is to compute the `n x n` covariance matrix for the n-dimensional dataset. This matrix captures all the variance and covariance information between the different dimensions.

#### 2. Find the Eigenvalues and Eigenvectors of the Covariance Matrix
This is the mathematical core of PCA.
-   A vector `V` is an **eigenvector** of a matrix `M` if, when multiplied by `M`, its direction does not change. It is only scaled by a scalar value `λ`, which is the corresponding **eigenvalue**. The relationship is `MV = λV`.
-   **The eigenvectors of the covariance matrix are the principal components.**
-   **The eigenvalue associated with each eigenvector represents the magnitude of the variance in that direction.**
-   Therefore, the eigenvector with the highest eigenvalue is the first principal component (PC1), the one with the second-highest eigenvalue is the second principal component (PC2), and so on.

#### 3. Project the Data onto the New Lower-Dimensional Space
Once we have the principal components (eigenvectors), we choose the top `k` components that we want to keep. We then form a **projection matrix** from these `k` eigenvectors.
-   To transform our original n-dimensional data into the new k-dimensional space, we multiply the data by this projection matrix.

#### 4. Optionally, Reconstruct the Data
To evaluate how much information was lost, we can perform an **inverse projection**. This takes the k-dimensional projected data and transforms it back into the original n-dimensional space. The reconstructed data will not be identical to the original (unless we kept all dimensions), but it should be a close approximation.

We can measure the difference between the original and reconstructed data using metrics like:
-   **Mean Squared Error (MSE):** The average squared difference between the original and reconstructed points.
-   **Coefficient of Determination (R²):** Measures the proportion of the variance in the original data that is preserved in the reconstructed data.

### Case Study: Eigenfaces
A famous application of PCA is in facial recognition, known as "Eigenfaces."
-   **Dataset:** Multiple images of faces, each resized to a standard size (e.g., 128x128 pixels).
-   **Dimensionality:** Each pixel is treated as a dimension. For a 128x128 image, this results in **16,384 dimensions**.
-   **PCA Application:** PCA is applied to this massive dataset. The resulting principal components (eigenvectors) are themselves vectors of 16,384 values, which can be visualized as 128x128 images. These are the "eigenfaces."
-   **Result:** The eigenfaces look like ghostly, generic facial templates. The first few eigenfaces capture the most common variations across all faces in the dataset (e.g., general face shape, lighting from the side). It turns out that a small number of these eigenfaces (e.g., a few dozen) can capture over 80% of the variation in the entire dataset.
-   **Reconstruction:** By projecting an original face image onto these top few eigenfaces and then reconstructing it, we get a recognizable but slightly blurry version of the original. This demonstrates that we have successfully compressed the essential "face" information from 16,384 dimensions into a much smaller number, effectively performing dimensionality reduction.


# Lecture 8 - Data Clustering

---

## Contents
- Data vs. Information and Learning Methods
- Introduction to Clustering
- Similarity Metrics
- K-means Clustering
- Hierarchical Clustering
- Expectation-Maximization (E-M) Clustering
- DBSCAN: Density-Based Clustering

---

## Data vs. Information and Learning Methods

To understand clustering, we must first distinguish between data and information.

-   **Data:** Raw, generally unorganized, and unstructured facts, observations, numeric values, or symbols. By itself, data does not have a specific meaning or purpose.
-   **Information:** Structured and meaningful knowledge derived from data. Information has context, relevance, and a specific purpose, allowing for one or multiple interpretations.

Information is the result of processing data, and this processing often involves a **learning mechanism**. Learning is the process of obtaining information from data through techniques like searching, pattern matching, feature extraction, and classification.

### Types of Learning Methods

There are two primary types of machine learning methods:

-   **Unsupervised Learning:** This approach involves searching through data to find hidden patterns, features, or natural groupings without any prior knowledge or labeled examples. All processing is based on features obtained directly from the input data itself. Clustering is a form of unsupervised learning.
-   **Supervised Learning:** This method uses pre-existing, pre-processed, labeled data (called "training data") to learn how to recognize similar patterns or elements in new, unseen data. Most processing in supervised learning relies on parameter values that were deduced by analyzing this known data.

---

## Introduction to Clustering

**Clustering** is a fundamental **unsupervised learning** technique. Its primary goal is to **subdivide data into clusters**, which are groups of data items formed based on their **similarity** to one another.

-   A **cluster** is a subset of the data wherein the elements share common or similar features.
-   Each group or cluster may constitute a separate category. In this way, clustering classifies data items into categories according to how they are naturally grouped.
-   The core of clustering is the concept of **similarity**, which is a formula or metric that determines how "close" two data items are in terms of the potential information they can provide.

### Potential Uses of Clustering

Clustering has a wide range of applications across many domains:

-   **Identify individuals based on similar preferences or histories:**
    -   Grouping online customers based on their browsing and purchasing history for targeted marketing.
    -   Segmenting users of a service based on their usage patterns and ratings.
    -   Categorizing hospital patients based on their medical history to identify risk groups.
-   **Filter data sets based on correlation:**
    -   **Remove redundancies:** Identify repeating items that do not contribute to new information gain.
    -   **Identify outliers:** Find isolated data items that do not conform to the general "trend" of the data set.
-   **Determine the optimal placement of items in a certain area:**
    -   Placing sensors, signal towers, or administrative buildings to ensure proper coverage of services by analyzing the clusters of user activity.

---

## Similarity Metrics

A crucial aspect of clustering is the method used to quantify how close two items are. The technical term for this is a **similarity metric**.

A metric is a function that:
-   Takes two items of the same type as input.
-   Returns a numeric value as output.

This output quantifies a relationship between the two items, determined by their specific features or attributes. A smaller distance or a higher similarity score indicates that two items are more similar.

### Examples of Similarity Metrics

#### For Numerical Data
Consider two items, A and B, represented as vectors of `n` features: A = {p₁, p₂, ..., pₙ} and B = {q₁, q₂, ..., qₙ}.

-   **Euclidean Distance:** The straight-line or "as the crow flies" distance between two points.
    -   `d = √Σ(pᵢ - qᵢ)²`
-   **Manhattan Distance:** The "city block" distance, calculated as the sum of the absolute differences of their coordinates.
    -   `d = Σ|pᵢ - qᵢ|`
-   **Minkowski Distance:** A generalized form that includes both Euclidean (p=2) and Manhattan (p=1) distances.
    -   `d = (Σ|pᵢ - qᵢ|ᵖ)¹/ᵖ`

#### For String Data
-   **Hamming Distance:** Used for strings of equal length. It counts the number of positions at which the corresponding characters are different.
    -   Example: The Hamming distance between "EUCLIDE**A**N" and "MANHATT**A**N" is 7.
-   **Levenshtein Distance:** Measures the number of single-character edits (insertions, deletions, or substitutions) required to change one word into the other.
    -   Example: The Levenshtein distance between "H**Y**UNDA**I**" and "H**O**NDA" is 3 (one substitution, two deletions).

---

## K-means Clustering

K-means is one of the most popular and widely used clustering algorithms.

-   **Purpose:** To partition `n` observations into a pre-determined number (`K`) of clusters (C₁, C₂, ..., Cₖ).
-   **Objective:** To form clusters such that the **within-cluster variation is minimized**. The algorithm searches for a partition that minimizes the sum of all variations across all clusters.

### The Objective Function
The number of clusters, `K`, must be decided beforehand. For a given distance metric like the Euclidean distance, the within-cluster variation `CV(Cₖ)` is defined as the average squared distance among all pairs of elements in the cluster.

The goal is to minimize: `Σ CV(Cₖ)` for `k=1 to K`.

Checking every possible combination of clusters is computationally impractical. The number of ways to partition `n` objects into `k` groups is given by the *Stirling numbers of the second kind*, which grows astronomically. For just `N=25` points and `K=4` clusters, there are over `5 * 10¹³` possible combinations.

### The K-means Algorithm
To solve this problem efficiently, the objective is reformulated:

> **Find the cluster partition that minimizes the sum of distances between each point and its corresponding cluster centroid.**

A **centroid** is the barycenter or average of all the points in a cluster. The basic algorithm is an iterative process:

1.  **Initialization:** Randomly generate an initial partition by assigning each data point to one of the `K` clusters.
2.  **Repeat:**
    a. **Update Step:** For each of the `K` clusters, compute its new centroid (the mean of all points currently in that cluster).
    b. **Assignment Step:** For each data point, assign it to the cluster whose centroid is closest.
3.  **Termination:** The algorithm stops when the cluster assignments no longer change between iterations.

### Initialization and Choosing K
The final clusters produced by K-means are highly dependent on two factors: the **initialization** and the chosen **number of clusters, K**.

#### Initialization Methods
Simple random assignment is often ineffective. Better methods aim to place initial centroids as close as possible to the optimal final centers to reduce the number of iterations needed.
-   **Sharding:** Sort all data points based on the sum of their attributes. Split the sorted list into `K` equal-sized subsets ("shards"). The initial centroid for each shard is the mean of its points.
-   **K-means++:** A smarter initialization that aims to spread centroids far apart.
    1.  Choose the first centroid randomly from the data points.
    2.  For each subsequent centroid, choose a new point `x` with a probability proportional to `D(x)²`, where `D(x)` is the distance from `x` to the *nearest existing centroid*.

#### Choosing the Optimal Number of Clusters (K)
-   **Elbow Method:** Run K-means for a range of `K` values (e.g., 1 to 10) and plot the Sum of Squared Errors (SSE) for each `K`. The SSE is the sum of squared distances between each point and its cluster's centroid. The graph will typically form an "elbow." The value of `K` at the point where the rate of SSE decrease abruptly slows is considered the optimal `K`.
-   **Average Silhouette Method:** This method measures the quality of clustering by evaluating how well-separated the clusters are. The silhouette score for a single point ranges from -1 to 1.
    -   A score close to **+1** indicates the point is far from neighboring clusters (good separation).
    -   A score close to **-1** indicates the point is close to the boundary of a neighboring cluster (poor separation).
    The `K` that yields the highest average silhouette score across all points is often the best choice.

---

## Hierarchical Clustering

While K-means is powerful, it requires specifying `K` beforehand and is sensitive to random initialization. **Hierarchical clustering** offers a deterministic alternative that does not require these parameters.

### Agglomerative Clustering
This is the most common "bottom-up" approach to hierarchical clustering.
1.  **Initialization:** Start by placing each data point in its own cluster.
2.  **Repeat:**
    a. Find the two closest clusters in the dataset.
    b. Merge them into a single, new, higher-level cluster.
3.  **Termination:** The process continues until all elements have been merged into a single, all-encompassing cluster.

The resulting hierarchy can be visualized using a **dendrogram**, a tree-like diagram where the leaves are the individual data points and the length of the branches is proportional to the distance between the clusters being merged.

### Linkage: Measuring Cluster Dissimilarity
The key to hierarchical clustering is defining the distance (or **dissimilarity**) between *clusters*, not just individual points. This measure is called **linkage**.
-   **Single Linkage:** The dissimilarity is the distance between the *closest* pair of points, one from each cluster.
-   **Complete Linkage:** The dissimilarity is the distance between the *farthest* pair of points.
-   **Average Linkage:** The dissimilarity is the *average* distance between all pairs of points formed from the two different clusters.
-   **Centroid Linkage:** The dissimilarity is the distance between the centroids of the two clusters.

---

## Expectation-Maximization (E-M) Clustering

Expectation-Maximization (E-M) can be seen as a **probabilistic** or "soft" version of K-means.
-   **K-means:** A point belongs exclusively to one cluster (hard assignment).
-   **E-M:** A point belongs to **all clusters** with certain probabilities (soft assignment).

In E-M, each cluster is described by a probability distribution (e.g., a normal distribution with a mean `μ` and standard deviation `σ`). The goal is to find the best parameters for these distributions.

### The E-M Algorithm
1.  **Initialization:** Initialize the cluster parameters (`μₖ` and `σₖ` for each cluster `k`) randomly.
2.  **Repeat:**
    a. **Expectation (E-step):** For each point `xᵢ`, calculate its "responsibilities" (`θᵢₖ`). This is the probability that point `i` belongs to cluster `k`, given the current cluster parameters.
    b. **Maximization (M-step):** Update the parameters (`μₖ` and `σₖ`) for each cluster using a weighted average of all data points. The weight for each point is its responsibility for that cluster, as calculated in the E-step.
3.  **Termination:** The algorithm converges when the cluster parameters no longer change significantly between iterations.

---

## DBSCAN: Density-Based Clustering

**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** is a powerful algorithm that defines clusters as dense regions of points, separated by regions of lower density.

### Core Concepts
DBSCAN relies on two key parameters:
-   `ε` (epsilon): The radius of the neighborhood to consider around any given point.
-   `minPts`: The minimum number of points required within a point's `ε`-neighborhood for it to be considered part of a dense region.

Based on these parameters, points are classified as:
-   **Core Point:** A point that has at least `minPts` within its `ε`-neighborhood.
-   **Border Point:** A point that is not a core point itself but is within the `ε`-neighborhood of a core point.
-   **Noise Point (Outlier):** A point that is neither a core nor a border point. It is isolated in a low-density region.

A cluster is then formed by connecting all reachable points. Two points are considered **reachable** if there exists a path between them consisting entirely of core points.

### Advantages of DBSCAN
The primary advantage of DBSCAN is its ability to find **clusters of arbitrary shapes**. Unlike K-means, which assumes clusters are spherical, DBSCAN can identify complex structures like concentric circles or winding paths. It is also inherently robust to outliers, which are automatically classified as noise.

# Lecture 9 - Supervised Classification

---

## Contents
- Introduction to Data Classification
- Supervised vs. Unsupervised Classification
- The Supervised Classification Process
- Important Concepts in Classification
- Linear Classification
- K-Nearest Neighbors (KNN)

---

## Introduction to Data Classification

Data classification is a fundamental task in data analysis and machine learning. Its primary goals are to:
-   **Organize data into categories** (also known as **classes**).
-   **Extract relevant information** from raw data.
-   **Provide meaning to raw data elements** by assigning them a semantic interpretation, which defines what the data represents.

### Examples of Classification
Classification is used in a vast array of applications:
-   Classifying images according to their content (e.g., 'cat', 'dog', 'car').
-   Classifying an email as 'spam' or 'not spam'.
-   Classifying actions or decisions as 'valid', 'invalid', or 'acceptable'.
-   Classifying a sequence of bytes as 'useful' or 'corrupt'.

### Classification as a Learning Process
At its core, **classification is a learning process**. A classification method is an algorithm that:
1.  **Analyzes** the data.
2.  **Discovers** relevant data attributes or features.
3.  **Maps** data items to specific classes based on those discovered features.

| Data Type          | Possible Features                                                               | Possible Classes (What the data might represent)            |
| ------------------ | ------------------------------------------------------------------------------- | ----------------------------------------------------------- |
| **Images**         | Segmented regions, pixel sums, contours, filtering, frequency components        | Faces, persons, vehicles, text, various other objects       |
| **Text**           | Patterns, repeating sequences, keywords, character/word counts                  | Specific messages, spam, advertising, names, bits of info   |
| **Personal Data**  | Usage patterns, action history, preferences                                     | Certain types of users, psychological profiles              |

---

## Supervised vs. Unsupervised Classification

Classification methods can be broadly divided into two main paradigms.

### Unsupervised Classifiers
-   **Process:** These methods search for and analyze patterns, relationships, and correlations *within* the data itself, without any prior knowledge.
-   **Goal:** They identify a class for each data item based purely on the results of this internal analysis.
-   **Example:** Clustering methods, which we discussed previously. In this context, each cluster discovered by the algorithm becomes a class, and every item within that cluster belongs to the same class.

### Supervised Classifiers
-   **Process:** These methods learn patterns, relationships, and correlations from **already classified data**.
-   **Requirement:** To be useful, they must first analyze a pre-classified data set, known as **"training data."**
-   **Goal:** They adjust their internal structure and parameters based on what is learned from the training data.
-   **Application:** Once trained, they apply the resulting rules to classify new, unclassified data of the same type.

---

## The Supervised Classification Process

Supervised classification involves a series of well-defined steps and concepts.

### Common Concepts and Notations
-   **Input Data (X):** The data to be classified is expressed as a set of attribute values, `X = {x₁, x₂, ..., xₙ}`. These attributes could be the pixels of an image, the coordinates of a point, or the personal details of an individual.
-   **Classes (y):** The categories are known in advance, and there is a finite number of them. For example, the classes for digit recognition are `{0, 1, ..., 9}`. Classes are typically encoded as numeric labels, e.g., `y ∈ {0, 1, ..., nrClasses-1}`.
-   **Training Data:** A set of input items `Xᵢ` for which the correct class labels `yᵢ` are already known. This set is represented as pairs `(Xᵢ, yᵢ)`.
-   **Classifier:** An algorithm that takes an input `X` and generates a label `y`.

### The Purpose of Training
The goal of training a supervised classifier is to **identify the rules, correlations, and relationships** between the domain of the input data `X` and the domain of the output classes `y`. These rules are learned from the training data so that any new, unseen `X` can be matched with the most appropriate label `y`.

### The Full Classification Workflow
1.  **Establish Data Type:** Define the kind of data to be processed (images, text, etc.).
2.  **Establish Classes:** Define the categories into which the data will be divided.
3.  **Collect Training Data:** Obtain a sufficiently large and relevant set of already-classified data items.
4.  **Decide on the Classification Algorithm (the Classifier):** Choose the model and logic that will be used for classification.
5.  **Train the Classifier:** Use the training data to build the structure of the classifier and adjust the parameters of the chosen model.
6.  **Evaluate the Classifier:** Test how well the trained model performs and measure its classification error.
7.  **Exploit the Classifier:** Apply the trained and validated model to new, unclassified data to assign them to their corresponding classes.

---

## Important Concepts in Classification

### The Classification Model
A classification model is a collection of functions, rules, and relationships that attempts to describe the classification problem mathematically. Its behavior is controlled by multiple parameters. **Training the model** is the process of adjusting these parameters based on the training data so that the model best describes the correlation between the training inputs and their known labels.

### Vector Space, Decision Regions, and Boundaries
-   **Vector Space:** The domain of all possible values for the input data `X`. It is an n-dimensional space where `n` is the number of attributes.
-   **Decision Region:** A region within the vector space where all points are assigned to the same class. It is an n-dimensional volume.
-   **Decision Boundary:** An n-dimensional surface that separates the different decision regions.

The purpose of a classifier is to analyze the training data and learn the optimal **decision boundary** that splits the entire vector space into decision regions for each class. Training involves finding the "best" separation contour based on the available points.

### Overfitting
A well-determined decision surface should separate the classes with minimal error, but its shape should not be *excessively complicated*.

**Overfitting** occurs when a classifier tries to fit the decision surface to the training data too closely.
-   An overfit model will classify the existing training points perfectly, avoiding any errors on the data it has already seen.
-   **However**, it will perform poorly when classifying new points because its decision regions are too complex and tailored to the specific noise and quirks of the training set. It fails to generalize.

Overfitting can be caused by:
-   A model that is too complex (too many parameters).
-   Poor quality training data (affected by noise, outliers, or errors).
-   Insufficient training data.

---

## Linear Classification

A linear classifier is one where the decision boundary is a line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions).

-   **Model:** The prediction is based on a linear combination of the data attributes:
    `y = f(w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ)` or in vector form, `y = f(WᵀX)`
-   **Parameters (W):** The weights `wᵢ` are the parameters of the model, learned during training.
-   **Decision Rule (f):** The function `f` makes the final class choice. For two classes coded as -1 and 1, this is commonly the **signum function**: `sign(x)`.

Linear classification works well for data that is **linearly separable**, meaning there exists a straight line or plane that defines a good decision boundary between the classes.

### Training a Linear Classifier
Training is an optimization problem. The goal is to find the weights `wᵢ` that minimize the **loss**, which is the difference between the predicted label (`ŷ`) and the true label (`y`).

This is typically solved numerically using **Gradient Descent**:
1.  **Initialize:** Randomly initialize the weight parameters `wᵢ`.
2.  **Repeat:**
    a. Determine the gradient (the direction of steepest ascent) of the loss function with respect to each weight `wᵢ`.
    b. Adjust the weights by taking a small step in the opposite direction of the gradient, scaled by a **learning rate `α`**.
3.  **Terminate:** Stop when the loss no longer changes significantly or after a specified number of iterations.

Common **loss functions** include:
-   **Zero-one loss:** 1 if `ŷ ≠ y`, 0 otherwise.
-   **Mean squared error (MSE):** `(1/n) * Σ(yᵢ - ŷᵢ)²`
-   **Cross-entropy:** `-Σ(yᵢ * log(ŷᵢ))`

---

## K-Nearest Neighbors (KNN)

KNN is a simple yet powerful distance-based classification method. It does not explicitly define a classification model but rather infers the class of a new point based on its neighbors.

-   **Principle:** A point is classified according to the **dominant class** of its `K` most similar neighbors.
-   **Requirement:** It requires a similarity or distance metric suitable for the data type.

### How KNN Works
To classify a new, unclassified point:
1.  Calculate the distance from the new point to every point in the training dataset.
2.  Identify the `K` closest points (the "nearest neighbors").
3.  Assign the new point to the class that is most common among those `K` neighbors.

-   **Non-weighted KNN:** The dominant class is determined by a simple majority vote. If `K=3` and two neighbors are 'red' and one is 'green', the new point is classified as 'red'.
-   **Weighted KNN:** The vote of each neighbor is weighted by its distance. Closer neighbors have a greater influence. A common weighting scheme is `W = 1 / distance²`. Instead of counting neighbors, the algorithm sums the weights for each class, and the class with the highest total weight wins.

### Evaluation and Representation
-   **Evaluation:** The classification error is the number of incorrectly classified points divided by the total number of points.
-   **Representation:** For `K=1`, the decision boundaries form a **Voronoi diagram**, where each region belongs to a single training point. For larger `K`, the decision boundaries become smoother and more complex.

# Lecture 10 - Statistical Data Analysis

---

## Contents
- The Process and Importance of Data Analysis
- Random Variables and Probability Distributions
- Popular Probability Distributions
- Probability Distribution Fitting
- Monte Carlo Estimation

---

## The Process and Importance of Data Analysis

**Data analysis** is a comprehensive process that involves several key activities to extract meaningful insights from raw data. These activities include:

-   **Exploring:** Traversing a body of data and iterating through its elements to understand its basic structure.
-   **Filtering:** Extracting useful data while discarding unusable or corrupted data.
-   **Transforming:** Changing the data so that it can be interpreted in a more usable manner, such as changing its coordinate system or domain.
-   **Modeling:** Developing rules based on the data. This involves creating a mathematical interpretation of the data that describes its behavior, tendencies, and variation.

### The Importance of Statistics
Real-world data is rarely perfect. It has inherent limitations that must be addressed, which is why statistics are crucial. Real-world data is:

-   **Finite:** We often only have a limited number of data elements available.
-   **Limited:** The available data may not fully represent the entire domain of possible values.
-   **Partially Irrelevant:** Not all available data is usable or meaningful for a specific analysis.
-   **Uncertain:** The processes of data acquisition, measurement, and collection are susceptible to inaccuracies, errors, and inconsistencies. There is always uncertainty about the quality of the data.

Because of these limitations, data analysis methods must account for uncertainty. Instead of making absolute statements like "event E occurs," we use a statistical approach: **"there is a chance that event E may occur."**

**Statistics** is the practice of working with **probabilities** instead of actual, fixed values.
-   **Non-statistical statement:** `x = 5`
-   **Statistical statement:** `P(x=5) = 0.7` (There is a 70% chance that the value of x is 5).

---

## Random Variables and Probability Distributions

At the heart of statistics is the concept of a **random variable**. Unlike a fixed variable, a random variable does not have a single, defined value. Instead, each of its possible values has a certain probability of occurring.

-   A **fixed, non-random variable** is described by its value.
-   A **random variable** is described by:
    -   Its **value domain** (the set of all its potential values).
    -   The **probabilities** for each potential value.

The set of probabilities that a random variable has across its domain is called its **probability distribution**. This distribution can be:
-   **Discrete/Empirical:** Probabilities are defined for a finite set of possible values (e.g., `P(age=10) = 0.4`).
-   **Continuous:** Probabilities are defined over a continuous, infinite domain (like all real numbers), typically by a function that gives the probability of a value falling within a certain subdomain.

### Probability Distribution Functions
A probability distribution is described by two key functions:

-   **Probability Density Function (PDF):** A function that describes the relative likelihood that a random variable will take on a given value. For a continuous variable, the area under the PDF curve over a certain interval gives the probability that the variable will fall within that interval.
-   **Cumulative Distribution Function (CDF):** A function that describes the probability that a random variable `X` will be found to have a value less than or equal to `x`. It represents the accumulated contribution of the domain, so `CDF(x) = P(X ≤ x)`.

---

## Popular Probability Distributions

Certain patterns of randomness occur frequently in the real world and are described by well-known probability distributions.

### Bernoulli Distribution
This is the simplest distribution, modeling an event with a binary outcome.
-   A random variable `X` can take one of two values:
    -   `1` (success), with probability `p`.
    -   `0` (failure), with probability `q = 1 - p`.
-   **Use Case:** Determines the probability of a single success/failure event, like a coin toss ("getting heads") or rolling a die ("getting a 1").

### Uniform Distribution
This distribution models a situation where all outcomes in a specified range are equally likely.
-   A random variable `X` has an equal probability of taking on any value from a specified range `[a, b]`.
-   The PDF is a flat line over the range `[a, b]`.

### Binomial Distribution
This distribution describes the probability of a specific number of successes in a fixed number of independent Bernoulli trials.
-   **Parameters:**
    -   `n`: The number of trials.
    -   `p`: The probability of success in a single trial.
-   **Random Variable:** The number of times the experiment was successful out of `n` attempts.
-   **Use Case:** What is the probability of getting exactly 2 heads in 3 coin tosses?

### Normal Distribution (Gaussian or "Bell Curve")
This is one of the most frequently encountered distributions in real-world applications.
-   **Parameters:**
    -   **Mean (μ):** The central value that has the highest probability of occurrence. It marks the peak of the bell curve.
    -   **Standard Deviation (σ):** A measure of how spread out the values are from the mean. A smaller `σ` results in a narrower, taller bell, while a larger `σ` results in a wider, shorter bell.
-   **Use Case:** Modeling natural phenomena like height, weight, measurement errors, and test scores.

### Poisson Distribution
This distribution models the number of times an event occurs within a specific time frame, space, or volume.
-   **Parameter:**
    -   **Lambda (λ):** The expected rate of occurrence.
-   **Random Variable:** The number of occurrences of an event.
-   **Use Case:** If a store expects an average of 3 customers per minute (λ=3), what is the probability that exactly 5 customers will walk in during a given minute?

---

## Probability Distribution Fitting

In the real world, we often have a limited set of experimental data and want to understand its underlying distribution. **Probability distribution fitting** is the process of finding a known parametric distribution (like a Normal distribution) that best describes our empirical data.

There are two ways to assess a distribution from data:
-   **Empirical Distribution:** A discrete list of probabilities determined directly from a finite set of known data (e.g., a normalized histogram).
-   **Parametric Distribution:** An estimated set of parameters (like `μ` and `σ` for a Normal distribution) for a known continuous distribution that best fits the empirical data.

### The Fitting Process
1.  **Build a histogram** of the experimental data to visualize its frequency.
2.  **Normalize the histogram** by dividing each bin's count by the total number of elements. This converts the frequency histogram into an empirical probability distribution.
3.  **Decide which type of distribution to fit.** If the histogram has a bell-like shape, we might attempt to fit a Normal distribution.
4.  **Find the best parameters** (`μ` and `σ` in the case of a Normal distribution) that cause the least error between the parametric curve and the normalized histogram. This is an **optimization problem**.
5.  **Evaluate the fitting error** using metrics to see how closely the parametric distribution matches the empirical one. Common metrics include:
    -   **Mean Squared Error (MSE)**
    -   **Chi-squared**
    -   **R-squared (Coefficient of Determination)**

---

## Monte Carlo Estimation

In many real-world applications, we use a mathematical **model** to process several inputs and produce outputs. If the inputs are not fixed values but are themselves random variables with known distributions, how can we determine the distribution of the output?

This problem is frequently solved using **Monte Carlo-based methods**.

### The Monte Carlo Method
Monte Carlo is a computational technique that relies on repeated random sampling to obtain numerical results. The core idea is to understand the behavior of a system by running many simulations.
1.  Start with a mathematical model: `output = f(input₁, input₂, ...)`.
2.  Retrieve multiple sets of values from the input distributions by generating random samples.
3.  Apply each set of sampled input values to the model to obtain a set of output values.
4.  Interpret the resulting set of output values to estimate the output distribution.

The more simulations you run, the more accurate the estimated output distribution becomes.

### Case Study: Estimating a Health Indicator
-   **Goal:** Estimate the distribution of a health assessment indicator (`ha`) for a population.
-   **Model:** `ha = f(bw, age, ex) = (bw * age) / ex`
-   **Inputs:** Body weight (`bw`), age, and exercise amount (`ex`) are not fixed values but are random variables, each with its own known Normal distribution.

**The Simulation Process:**
1.  Run a large number of simulations. For each simulation `i`:
    -   Generate a random sample for body weight (`bwᵢ`), age (`ageᵢ`), and exercise (`exᵢ`) from their respective Normal distributions.
    -   Apply the model to get a single output sample: `haᵢ = (bwᵢ * ageᵢ) / exᵢ`.
2.  After many simulations, use the collected set of `haᵢ` values to build a probability histogram (an empirical distribution of the health indicator).
3.  Attempt to fit a parametric Normal distribution to this empirical output distribution.

### Interpreting the Results
The output of the Monte Carlo simulation is an estimated probability distribution for the health assessment `ha`. We can now use this distribution to answer probabilistic questions.
-   **Rule:** Suppose a health risk exists if `ha < 50`.
-   **Question:** What is the probability of this health risk manifesting in the population?
-   **Answer:** We can calculate `P(ha < 50)` by finding the area under our estimated PDF curve for the interval from 0 to 50. This gives us a quantitative measure of the risk.


# Lecture 11 - Data Compression

---

## Contents
- Introduction to Data Compression
- Performance Metrics for Compression
- Lossless Compression Algorithms
    - Run-Length Encoding (RLE)
    - Huffman Coding
    - Lempel-Ziv (LZ) Compression
- Lossy Compression: DCT Compression

---

## Introduction to Data Compression

Data compression is the process of reducing the number of bits used to store and represent data. This is a fundamental technique in computer science with significant practical benefits:

-   **Reduced storage capacity:** Compressed files take up less space on disks and servers.
-   **Faster file transfer:** Smaller files can be transmitted more quickly over networks.
-   **Decreased costs:** Reduces expenses related to both storage hardware and network bandwidth.

### The Compression Process
The process always involves two steps that work together:

1.  **Compression:** Replacing a body of data with a smaller, encoded version.
2.  **Decompression:** Expanding the compressed representation to recover the original data, either partially or completely.

A matching compression-decompression pair is known as a **codec** (coder-decoder). The fundamental purpose of a codec is to **reduce redundancy** in data by searching for patterns and replacing those patterns with shorter symbols.

### Two Main Types of Compression

#### Lossless Compression
-   **Definition:** With lossless compression, **no information is lost**. The original data can be fully and perfectly recovered from its compressed version.
-   **Common Use Cases:** This method is essential for data where perfect fidelity is required, such as **text and numeric data**. A compressed text document must be restored exactly, otherwise missing bytes would result in corrupted or unintelligible data.

#### Lossy Compression
-   **Definition:** With lossy compression, **some information is permanently lost**. The decompressed data is only an **approximation** of the original.
-   **Common Use Cases:** This method is commonly used for **images, video, and audio**. For this type of media, perfect bit-for-bit recovery is often not necessary. The content can still be clearly visible or audible even if the decompressed file is not identical to the original. A small loss in quality is traded for a much larger reduction in file size.

### Comparing Lossless and Lossy Approaches
-   **Lossless compression** typically works by exploiting **statistical redundancies** in the data, such as repeating elements, common patterns, or portions of data with a predictable distribution.
-   **Lossy compression** usually works by applying a mathematical **transformation** to the data (e.g., from the time or spatial domain to the frequency domain) to obtain a set of transformation coefficients. It then **limits or reduces these coefficients** (this is the "lossy" step) and applies the inverse transformation to the limited set of coefficients to reconstruct an approximation of the data.

The most effective compression methods are often **hybrid**, using a lossless algorithm to reduce initial redundancy and then a lossy algorithm to compress the data further.

---

## Performance Metrics for Compression

We use several key metrics to evaluate the performance of a compression algorithm.

#### Symmetric vs. Asymmetric Compression
This refers to the relative speed of the compression and decompression processes.
-   **Symmetric Compression:** Compression and decompression occur at similar speeds. This is crucial for real-time scenarios like video or audio streaming.
-   **Asymmetric Compression:** Compression and decompression occur at different speeds. Typically, compression is much slower than decompression. This is well-suited for applications involving data storage and archival, where data is written once but read many times.

#### Compression Ratio
This metric measures how much the data has been compressed. It can be expressed in two ways:
-   **Ratio Format:** `(number of original bits) / (number of compressed bits)`. For example, compressing a 256 KB file to 64 KB yields a **4:1** compression ratio.
-   **Percentage Format:** The percentage by which the data size is reduced. A 4:1 ratio corresponds to a **75%** reduction, or a space savings of 75%.

#### Compression Rate
This is similar to the compression ratio but is data-type specific. It measures the number of bits required to store a single data element after compression (e.g., bits per pixel, bits per word).

#### Distortion
This metric applies only to **lossy compression** and measures the difference between the original and the decompressed data. **Rate-distortion theory** seeks to find the minimum number of bits per symbol required to allow the output to be approximately reconstructed within a given distortion tolerance.

---

## Lossless Compression Algorithms

### Run-Length Encoding (RLE)
RLE is a simple and intuitive lossless compression method.
-   **Principle:** It exploits redundancies by replacing "runs" (consecutive sequences) of repeating elements with a single `<element, count>` pair.
-   **Example:** The sequence `AABBBCDDDDDDDEEFGGGG` would be compressed as `A2B3C1D6E2F1G4`. For single elements, the '1' can be omitted, resulting in `A2B3CD6E2FG4`.
-   **Use Cases:** RLE is effective for data with long runs of identical values, such as simple binary images or continuous data streams that are frequently interrupted by long sequences of zeros or nulls.

### Huffman Coding
Huffman coding is a statistical encoding method that is fundamental to many lossless compression applications.
-   **Principle:** It exploits the uneven distribution of symbols in a sequence. The core idea is that **the more frequent a symbol is, the shorter its binary code should be**.
-   **Prefix Codes:** It generates **prefix codes** for each unique symbol. This is an important property where no code is a prefix of another code (e.g., if `01` is a code, no other code can start with `01`, like `010`). This allows for unambiguous decompression without needing separators.
-   **Implementation:** It is most commonly implemented by building a **Huffman tree**. The tree is constructed bottom-up by repeatedly combining the two least frequent symbols (or nodes) until a single root node is formed. The binary code for each symbol is then determined by traversing the tree from the root to the symbol's leaf, where a left branch is `0` and a right branch is `1`.

### Lempel-Ziv (LZ) Compression
The Lempel-Ziv family of algorithms is a highly popular and foundational set of lossless compression methods.
-   **Principle:** LZ is a **dictionary-based** method. It replaces a group of elements with a reference (an index) to a previous occurrence of that same group.
-   **Process:**
    1.  The algorithm parses the original sequence from left to right.
    2.  It finds the shortest subsequence that is not already present in its dictionary.
    3.  It outputs a reference to the prefix of that subsequence (which is in the dictionary) plus the final new character.
    4.  It adds the new, longer subsequence to the dictionary and assigns it an index.
-   **Decompression:** The compressed sequence is a list of instructions. The decompressor reconstructs the original data by looking up indices in the dictionary and appending the specified characters.

Many popular compression formats are based on LZ, including **LZW** (used in GIF, TIFF, PDF) and **DEFLATE** (which combines LZSS and Huffman codes and is used in ZIP, GZIP, PNG).

---

## Lossy Compression: DCT Compression

The most popular family of lossy compression methods is based on the **Discrete Cosine Transform (DCT)**. This is the core technology behind widely used image, video, and audio formats like **JPEG, MPEG, H.264, MP3, and AAC**.

### Transform Coding
The general method is called **transcoding** or **transform coding**. The process is:
1.  **Apply a transformation** to the uncompressed data. The most popular is the DCT, which transforms data from the spatial or time domain into the frequency domain.
2.  **Produce coefficients:** The result of the transform is a series of coefficients representing the frequency components of the data.
3.  **Quantize the coefficients:** This is the lossy step. **Quantization** limits the domain or number of values for the coefficients, typically by dividing them by values from a quantization table and rounding. This step introduces error but also makes many coefficients zero.
4.  **Decompression:** Reconstruct an approximation of the data by applying the inverse transform to the limited set of quantized coefficients.

### The DCT Compression Process
Let's consider the example of JPEG compression:
1.  **Start with samples:** The image is represented by its pixel values.
2.  **Divide into batches:** The image is divided into small `8x8` pixel blocks.
3.  **For each batch:**
    a. **Apply 2D DCT:** The DCT is applied to the `8x8` block, transforming the 64 pixel values into 64 frequency coefficients. Most of the signal's energy is concentrated in a few low-frequency coefficients (at the top-left of the block).
    b. **Quantize:** Each of the 64 coefficients is divided by a corresponding value in an `8x8` quantization table and rounded. High-frequency coefficients are often divided by larger numbers, causing many of them to become zero.
    c. **Retain non-zero coefficients:** After quantization, only a few non-zero coefficients remain. These, along with their positions, are what is actually stored. The fewer the coefficients kept, the higher the compression ratio, but the more information is lost.
4.  **Decompression:** The process is reversed. The quantized coefficients are de-quantized, and the inverse DCT is applied to reconstruct an approximation of the original `8x8` pixel block.

This process can introduce visible errors known as **compression artifacts** (e.g., blockiness or ringing). These can be mitigated by using a less aggressive quantization table (which results in a larger file size) or by applying post-processing filters like blur or sharpen.



# Lecture 12 - Stream Processing

---

## Contents
- Introduction to Stream Processing
    - Batch vs. Stream Processing
    - Usefulness and Characteristics of Stream Processing
- The Data Stream Management System (DSMS)
    - Key Components and Concepts
    - Data Stream Models
- Streaming Methods
    - Sliding and Tumbling Windows
    - Ageing and Sampling
- Challenges in Stream Processing

---

## Introduction to Stream Processing

As we've discussed, Big Data is characterized by its massive size, with processing needs often in the petabyte range. This continual increase in data volume means that traditional algorithms and storage solutions are often inadequate, necessitating dedicated resource management systems.

When it comes to managing the memory and computational load of Big Data, there are two main approaches: batch processing and stream processing.

### 1. Batch Processing
Batch processing is the traditional and most frequently used approach.
-   **Process:** Data is first collected and aggregated into large groups called **batches**. A batch is then processed within a certain time frame, after which the results are delivered.
-   **Characteristics:**
    -   It is **not real-time**.
    -   It is useful when the processing tool needs access to a large body of data—or all of it—at once.
    -   It is typically carried out by a parallel/distributed system. While it can be done sequentially, this is often too slow for real-world applications.
-   **The quintessential batch processing framework is MapReduce.**

The primary use case for batch processing is analyzing a large, static body of data, such as millions of records collected over a long period. Many analytical tasks can be performed very efficiently in batches because the operations can be highly parallelized (e.g., using mappers and reducers). However, this model requires a significant amount of data to already be available, a situation that is becoming less common as more data is generated and needs to be processed continuously.

### 2. Stream Processing
Stream processing is a paradigm designed to handle data in motion.
-   **Process:** Data is processed **as soon as it is collected or generated**. Computation happens in **almost real-time** (on the order of seconds or milliseconds).
-   **Characteristics:**
    -   There is a **minimal delay** between data input and getting results.
    -   It is useful when data is fed as a **continuous flow** that is constantly changing.

### Comparing Batch and Stream Processing

| Feature      | Batch Processing                       | Stream Processing                             |
|--------------|----------------------------------------|-----------------------------------------------|
| **Input**    | Data groups (batches)                  | Stream of new data, updates                   |
| **Data Size**| Known and finite (per batch)           | Potentially infinite, unknown in advance      |
| **Hardware** | Multiple processors                    | Limited to a small amount of memory           |
| **Storage**  | Large data store (e.g., HDFS)          | Not stored, or only a small portion in memory |
| **Processing**| Processed in multiple rounds           | A single pass (or very few passes)            |
| **Time**     | Extensive (hours, days, or longer)     | A few seconds or milliseconds                 |

---

## Usefulness and Characteristics of Stream Processing

Stream processing is suitable for data that is not only large in volume but also has **high velocity**—meaning it is acquired dynamically and arrives at a fast rate. This creates a need to react to incoming data in real-time, with very low latency.

This requires a different approach called **stream computing**, where static operations are carried out continuously on real-time data.

### Common Characteristics
-   **Single Pass:** Most commonly, stream processing involves a **single pass** through the data. The analytical model and its parameters are updated as new data arrives.
-   **No Full Storage:** The system **does not store all input into memory**. Data is handled continuously at a high rate. Once an element is processed, it is either discarded, ignored, or put into long-term archival storage.
-   **Multiple Sources:** Data streams are not limited to a single source or flow. A stream processor must be able to handle data from multiple sources, of different types, arriving asynchronously. Its job is to figure out how to sample, organize, transform, and obtain useful information from all this incoming data. Potential sources include databases, GPS devices, IoT sensors, user input, social media feeds, and mobile devices.

### Definition of a Data Stream

> A data stream is a **continuous** and potentially **infinite stochastic** process in which events occur **independently** from one another.

-   **Continuous:** The processor waits for data in an uninterrupted manner, treating it as a flow rather than discrete batches.
-   **Infinite:** The processor does not know when the stream will end, so it must be prepared to run indefinitely.
-   **Stochastic:** Data often arrives randomly with no predefined structure. The processor must be able to sample and organize the data as it comes in.
-   **Independent Events:** The processing of the current data element may not relate to previous processing. Multiple incoming streams are often unrelated, and it is the job of the processor to find or create connections.

---

## The Data Stream Management System (DSMS)

A DSMS is the architecture designed to handle the complexities of stream processing.

### Key Components and Concepts
-   **Input Stream(s):** Incoming data from one or multiple sources.
-   **Stream Processor:** The core logic engine that takes the input streams and applies an underlying logic to compute information from them.
-   **Working Storage:** A small, fast buffer used to store data that is currently undergoing processing.
-   **Archival Storage:** A larger, more permanent storage used to keep useful data from the stream. This is used for approaches that require some historical knowledge, but typically less than 1% of the total stream data is kept here.
-   **Streaming Query (Continuous Query):** A "standing" query that executes continuously for all incoming data elements. It updates its results in real-time as new data arrives, without needing to be re-issued. This is in contrast to a typical database query, which runs once on a static dataset.
-   **Ad-hoc Query:** A query issued dynamically when needed for specific, unscheduled information retrieval.
-   **Output Stream(s):** The stream of results from the stream processor. It may be structured differently from the input and contains the new information (features, statistics, analytics, etc.) obtained by transforming the input data.

### Data Stream Models
The "model" refers to the algorithms and methodology applied to the incoming data. Because it's impossible to store the entire stream, these algorithms must make trade-offs. They often:
-   Provide **approximate answers** (statistical results).
-   Use very **low memory buffers**.
-   Rely on sources of independent random elements, meaning that different runs may provide slightly different results, but most runs should be **approximately correct**.

Common models for handling incoming data elements include:
-   **Insert-only Model:** Once an element is processed, it cannot be changed.
-   **Insert-update Model:** A new element may overwrite or update an existing data item already in storage.
-   **Additive Model:** Each incoming element represents an increment or change to a previous version of a data object.

---

## Streaming Methods

A **streaming algorithm** is the logic used to manage the flow of data into stream processors, balancing the challenge of huge data volumes against limited memory and computational power. Since not all data can be stored, we need efficient methods to keep only the most meaningful data.

### Sliding and Tumbling Windows
-   **Sliding Windows:** This method keeps the most recent elements in memory and discards older ones. The window "slides" over the data stream.
    -   **Time-based:** The window has a constant time duration (e.g., the last 5 minutes). The number of elements inside it can vary.
    -   **Sequence-based:** The window contains a constant number of elements. Its time duration can vary.
-   **Tumbling Windows:** This method processes data in discrete, non-overlapping chunks. Once the current sequence (window) is processed, the window "tumbles" to the next new sequence. There is no overlap of items.

### Ageing
This method involves building a compact **summary** of the incoming data that contains the most important information.
-   Only the summary is kept in memory; individual processed items are discarded.
-   The importance of the summary itself can be decreased over time by multiplying it by a **decay factor**, giving more weight to recent data.

### Sampling
This involves selecting only a subset of the incoming data. The goal is to generate a smaller dataset that has a similar structure and the same representative features as the full set.
-   **Reservoir Sampling:** A method to sample `k` elements uniformly from a stream of unknown size. Each item in the stream has an equal probability of being included in the final reservoir of `k` items.
-   **Min-wise Sampling:** For each item in the stream, generate a random tag (a number between 0 and 1). The item with the smallest tag is sampled. This can be extended to sample the `k` items with the lowest tags.

---

## Challenges in Stream Processing

### Memory-Related Challenges
-   The required storage could theoretically increase without bound.
-   Traditional memory handling algorithms that assume a finite dataset do not work well.
-   The amount of computation per element must be extremely low to ensure low latency and allow the processor to keep up with the stream.

### Query-Related Challenges
-   **Approximate Results:** Due to limited storage and window sizes, results are often statistically correct rather than absolutely correct. A high-quality approximate answer is often preferable to no answer at all.
-   **Historical Queries:** Queries that need to take past data into account are difficult because the full history of the stream is not stored. A limited history can be provided via a sliding window, but this is always a compromise between speed, memory, and accuracy.
-   **Ad-hoc Queries:** It is difficult to reference past data in ad-hoc queries because that data has usually been discarded. These queries can generally only reference future data.


# Lecture 13 - Data Visualization

---

## Contents
- The Role and Purpose of Visualization
- The Information Visualization Model
- Visual Encoding and Data Types
    - Univariate Data
    - Bivariate Data
    - Multivariate Data
    - Multidimensional Data
- Visualizing Complex Data Structures
    - Set Data
    - Networks
    - Trees
- Visualization of Text Documents

---

## The Role and Purpose of Visualization

Raw data itself rarely has intrinsic visual properties like color or shape. The fundamental challenge when working with data is to retrieve meaningful information and, if necessary, represent that information visually through images. This makes visualization a non-trivial problem, especially when dealing with large, multi-dimensional datasets that need to be represented in just a few images.

### Usefulness of Visual Representation
Visual representation serves three main purposes in data analysis:

-   **Exploration:** Visually traversing data to find representative and useful features. This is often the first step in understanding a dataset.
-   **Confirmation:** Formulating a hypothesis about the data and then using an adequate visual representation to confirm or reject that hypothesis.
-   **Presentation:** Working with data in a way that facilitates its interpretation, illustrating its key features and purpose to an audience.

#### Exploratory vs. Confirmatory Analysis
-   **Exploratory Data Analysis:**
    -   **Start:** No initial hypothesis about the data.
    -   **Process:** Search and analyze the data to find potentially useful information and patterns.
    -   **Result:** A hypothesis is extracted *from* the data.
-   **Confirmatory Data Analysis:**
    -   **Start:** One or more pre-existing hypotheses about the data.
    -   **Process:** Examine the hypotheses with a specific goal in mind.
    -   **Result:** The confirmation or rejection of the initial hypotheses.

#### Data Presentation
-   **Start:** The facts to be presented are already known in advance.
-   **Result:** The goal is to communicate the results of the analysis efficiently and effectively to an audience, as seen in news graphics or reports.

---

## The Information Visualization Model

Information visualization is the process of taking external-world data and creating a representation that allows for human interpretation. It is a computer-based method that can be broadly categorized into:

-   **Scientific Visualization:** Typically deals with physical data (e.g., medical scans, fluid dynamics).
-   **Information Visualization:** Typically deals with abstract, non-physical data (e.g., financial data, social networks).

A comprehensive model for information visualization involves a pipeline that transforms raw data into an interactive view.

**Raw Data → [Data Transformations] → Data Tables → [Visual Mappings] → Visual Structures → [View Transformations] → Views**

-   **Raw Data:** Comes in various formats and has no intrinsic visual properties.
-   **Data Tables:** An organized representation of the data, showing relations among elements and metadata.
-   **Visual Structures:** The mapping of data to spatial cues, marks, and graphical properties.
-   **Views:** The final rendered image, controlled by graphical parameters like position, scaling, clipping, and zooming.
-   **Human Interaction:** Provides the controls for a user to perform transformations at each stage of the pipeline.

### Raw Data Transformation
The first step is to convert raw data (e.g., a collection of text documents) into an organized representation like a data table. This involves extracting values and structures, such as creating word embeddings or metadata tables (length, author, date) from documents.

### Data Tables
Data tables consist of **cases/items** (rows) and **variables/attributes** (columns). Attributes can be of several types:
-   **Nominal (Categorical):** Unordered labels (e.g., names like 'Hans', 'Anna').
-   **Quantitative (Numerical):** Measurable quantities (e.g., age like '46', '15').
-   **Ordinal:** Ordered labels with a meaningful sequence but no fixed interval (e.g., 'ID-11111', 'ID-22222').

### Visual Mappings
This is the core of visualization: finding a visual encoding that effectively describes the structured data. We represent data through visual elements by mapping non-visual data attributes to corresponding visual properties.

-   **Marks:** The basic geometric elements in a visualization (e.g., points, lines, areas).
-   **Visual Channels:** The properties that control the appearance of marks (e.g., position, color, shape, size, tilt).

Different visual channels are effective for encoding different types of data.
-   **Magnitude Channels (for Ordered/Quantitative Attributes):** Position on a common scale, length, angle, and area are highly effective.
-   **Identity Channels (for Categorical Attributes):** Spatial region, color hue, and shape are highly effective.

---

## Visual Encoding and Data Types

The choice of visualization technique depends heavily on the structure and dimensionality of the data.

### Univariate Data (One Variable)
-   **Box Plot:** An excellent method for visualizing the distribution of a single quantitative attribute. It represents the median, lower and upper quartiles, and outliers, making it highly effective for showing probability distributions.

### Bivariate Data (Two Variables)
-   **Scatterplot:** The classic method for visualizing two dependent, quantitative attributes. It is used to illustrate distributions, outliers, correlations, and potential clusters.

### Multivariate Data (Multiple Variables)
-   **Bubble Chart:** A scatterplot enhanced with additional visual channels. For example, a third quantitative attribute can be mapped to the **point size**, and a categorical attribute can be mapped to the **point color**.
-   **Scatterplot Matrix (SPLOM):** A grid of scatterplots where each cell shows the relationship between a pair of attributes. This allows for the simultaneous visualization of all pairwise correlations in a dataset.
-   **Parallel Coordinates:** A technique for visualizing many variables at once. Instead of orthogonal axes, each attribute is given its own parallel vertical axis. A single data item is represented as a polyline that connects its values on each axis. This method is useful for examining interactions and recognizing patterns across many dimensions.
-   **Radar Chart (Star/Spider Plot):** Axes are arranged radially from a central point. Each data item is represented as a closed polyline, allowing for comparison across multiple attributes.
-   **Chernoff Faces:** An unconventional method based on icons. Each data item is represented by a cartoon face, where different quantitative attributes are mapped to facial characteristics like eyebrow angle, eye size, or mouth curvature. Humans are naturally adept at recognizing facial patterns, making this a potentially powerful (though quirky) method.

### Multidimensional Data (Two or More Independent Variables)
-   **Heatmap:** Used to visualize data with two independent variables (on the X and Y axes) where a third, quantitative attribute is represented by a **color range** at each `(x, y)` position.

---

## Visualizing Complex Data Structures

### Set Data
Used for classified data with multiple, potentially overlapping categories.
-   **Euler/Venn Diagrams:** Represent containment, intersection, and exclusion through closed curves. They are useful for a small number of sets.
-   **Radial Sets:** Designed for a large number of items. Sets are represented as regions arranged radially, and the intersections between sets are shown as links or ribbons between the regions.
-   **Parallel Sets:** Similar in layout to parallel coordinates, but for categorical data. Each axis represents a categorical attribute, boxes on the axis represent the categories, and flowing ribbons between them show the relationships and flow of items between categories.

### Networks
Used to describe systems like communication networks, social graphs, or the internet.
-   **Node-Link Diagram:** The most common representation, where nodes are drawn as points and links as lines connecting them.
-   **Adjacency Matrix:** A grid where rows and columns represent nodes, and a colored cell at `(i, j)` indicates a link between node `i` and node `j`.
-   **NodeTrix (Hybrid):** A powerful technique for large networks that combines both approaches. The overall network structure is shown as a node-link diagram, but dense communities or local connections are represented as embedded adjacency matrices.

### Trees
Visual structures that use connection and enclosure to encode hierarchical relationships.
-   **Rectilinear Layouts:**
    -   **Node-Link:** The classic top-down tree diagram.
    -   **Icicle Plot:** A space-filling variant where nodes are represented by stacked rectangles.
    -   **Treemap:** A space-filling method where hierarchical data is represented by nested rectangles.
-   **Radial Layouts:**
    -   **Node-Link:** A tree where nodes radiate out from a central point.
    -   **Concentric Circles (Sunburst):** A space-filling radial layout.

---

## Visualization of Text Documents

The objective of text visualization is to transform text information into a spatial representation to reveal patterns, understand structure, and see relationships between documents.

-   **Word Cloud / Tag Cloud:** Encodes document statistics (like word frequency) through the visual properties of characters, most commonly **font size**. Common "stop words" (`the`, `of`, `a`) are removed, and the remaining words are tightly packed.
-   **Phrase Nets:** Visualizes text patterns by representing words as nodes and the relationships between them (semantic, co-occurrence) as edges. The size of the word/node often corresponds to its frequency.
-   **Theme River:** A type of stacked area chart that represents thematic changes in a document collection over time. Each colored "river" represents a theme, and its width at any point in time indicates the theme's prevalence.